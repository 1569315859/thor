1
00:00:00,600 --> 00:00:05,640
（翻译：Allen, Adam）
（https://github.com/ivanallen/thor）
好啦，咱们开始上课
I'd like to get started today we're

2
00:00:05,640 --> 00:00:09,389
（翻译：Allen, Adam）
（https://github.com/ivanallen/thor）
今天咱们谈谈 GFS
gonna talk about GFS the Google file

3
00:00:09,389 --> 00:00:10,980
（翻译：Allen, Adam）
（https://github.com/ivanallen/thor）
也就是就是今天我们要读的论文《The Google File System》
system paper we read for today

4
00:00:10,980 --> 00:00:12,660
这也是在咱们这门课里
and this will be the first of a number

5
00:00:12,660 --> 00:00:15,540
众多 case 里要学的第一篇
of different sort of case studies we'll

6
00:00:15,540 --> 00:00:17,160
关于如何构建大型存储的文章
talk about in this course about how to

7
00:00:17,160 --> 00:00:19,410
关于如何构建大型存储的文章
be build big storage systems so the

8
00:00:19,410 --> 00:00:29,310
这里“大型存储”是一个很大的话题
larger topic is big storage the reason

9
00:00:29,310 --> 00:00:31,410
为什么是存储？
is the storage is turned out to be a key

10
00:00:31,410 --> 00:00:34,260
因为存储被证明是一种关键抽象
abstraction you might you know if you

11
00:00:34,260 --> 00:00:35,850
如果你还不知道的话
didn't know already you might imagine

12
00:00:35,850 --> 00:00:37,230
你可以想象在分布式系统中
that there could be all kinds of

13
00:00:37,230 --> 00:00:40,050
你希望使用的各种不同的抽象
different you know important

14
00:00:40,050 --> 00:00:42,030
你希望使用的各种不同的抽象
abstractions you might want to use for

15
00:00:42,030 --> 00:00:43,650
但事实表明
distributed systems but it's turned out

16
00:00:43,650 --> 00:00:47,730
简单的存储接口往往更有用而且更加通用
that a simple storage interface is just

17
00:00:47,730 --> 00:00:50,010
简单的存储接口往往更有用而且更加通用
incredibly useful and extremely general

18
00:00:50,010 --> 00:00:51,480
构建分布式系统
and so a lot of the thought that's gone

19
00:00:51,480 --> 00:00:53,280
大多都是关于如何设计存储系统
into building distributed systems has

20
00:00:53,280 --> 00:00:55,170
或是设计其它类型的系统
either gone into designing storage

21
00:00:55,170 --> 00:00:57,630
在它的底层
systems or designing other systems that

22
00:00:57,630 --> 00:01:00,180
运行着一个不错的大型分布式存储系统
assume underneath them some sort of

23
00:01:00,180 --> 00:01:02,989
运行着一个不错的大型分布式存储系统
reasonably well behaved big just

24
00:01:02,989 --> 00:01:05,519
运行着一个不错的大型分布式存储系统
distributed storage system so we're

25
00:01:05,519 --> 00:01:07,500
所以我们会更加关注
going to care a lot about how the you

26
00:01:07,500 --> 00:01:09,360
如何为大型分布式存储系统设计一个优秀的接口
know how to design a good interface to a

27
00:01:09,360 --> 00:01:12,420
如何为大型分布式存储系统设计一个优秀的接口
big storage system and how to design the

28
00:01:12,420 --> 00:01:14,159
以及如何设计存储系统的内部结构
innards of the storage system so it has

29
00:01:14,159 --> 00:01:18,030
这样它才具备良好的行为
good behavior you know of course that's

30
00:01:18,030 --> 00:01:19,229
通过阅读这篇论文可以让我们起步
why we're reading this paper is to get

31
00:01:19,229 --> 00:01:20,850
通过阅读这篇论文可以让我们起步
a start on that. the this paper also

32
00:01:20,850 --> 00:01:22,530
论文也涉及到很多本课程常出现的话题
touches on a lot of themes that will

33
00:01:22,530 --> 00:01:24,900
论文也涉及到很多本课程常出现的话题
come up a lot in a tube for parallel

34
00:01:24,900 --> 00:01:27,060
主要包括并行性能、容错、复制和一致性
performance fault tolerance replication

35
00:01:27,060 --> 00:01:31,740
主要包括并行性能、容错、复制和一致性
and consistency and this paper is as

36
00:01:31,740 --> 00:01:34,140
论文的内容也像 Go 语言那样，简单明了
such things go reasonably

37
00:01:34,140 --> 00:01:36,390
论文的内容也像 Go 语言那样，简单明了
straightforward and easy to understand

38
00:01:36,390 --> 00:01:38,670
这也是一篇非常优秀的系统论文
it's also a good systems paper it sort

39
00:01:38,670 --> 00:01:40,560
从硬件到软件
of talks about issues all the way from

40
00:01:40,560 --> 00:01:43,229
到最终构建出整个系统都有涉及
the hardware to the software that

41
00:01:43,229 --> 00:01:45,960
到最终构建出整个系统都有涉及
ultimately uses the system and it's a

42
00:01:45,960 --> 00:01:49,320
而且在现实世界中 它也设计的相当成功
successful real world design so it says

43
00:01:49,320 --> 00:01:51,030
尽管这是在学术会议上发表的论文
you know academic paper published in an

44
00:01:51,030 --> 00:01:53,189
尽管这是在学术会议上发表的论文
academic conference but it describes

45
00:01:53,189 --> 00:01:54,890
但是文章里介绍的东西(GFS)也相当成功
something that really was successful and

46
00:01:54,890 --> 00:01:57,030
在现实世界中也使用了相当长的时间
used for a long time in the real world

47
00:01:57,030 --> 00:01:58,650
所以，在这里
so we sort of know that we're talking

48
00:01:58,650 --> 00:02:02,340
咱们讨论的这些东西 真的相当牛逼
about something that is it's a good a

49
00:02:02,340 --> 00:02:07,110
咱们讨论的这些东西 真的相当牛逼
good useful design okay so before I'm

50
00:02:07,110 --> 00:02:09,149
不过在讨论 GFS 前
gonna talk about GFS I want to sort of

51
00:02:09,149 --> 00:02:11,279
我想聊聊一点关于分布式存储系统的空间
talk about the space of distributed

52
00:02:11,279 --> 00:02:13,030
我想聊聊一点关于分布式存储系统的空间
storage systems a little bit

53
00:02:13,030 --> 00:02:18,810
首先 为什么它会如此之难？
to set the scene so first why is it hard

54
00:02:19,920 --> 00:02:23,560
你需要做大量的工作才能让他变得正确
it's actually a lot to get right but for

55
00:02:23,560 --> 00:02:25,900
但在 6.824 里
824 there's a particular sort of

56
00:02:25,900 --> 00:02:28,330
会有一些特殊的讲述方式
narrative that's gonna come up quite a

57
00:02:28,330 --> 00:02:32,140
在未来很多系统里你将会接触到
lot for many systems often the starting

58
00:02:32,140 --> 00:02:34,180
人们设计大型分布式系统
point for people designing these sort of

59
00:02:34,180 --> 00:02:35,890
或大型存储系统 通常的出发点是
big distributed systems or big storage

60
00:02:35,890 --> 00:02:37,330
为了获得巨大的综合性能
systems is they want to get huge

61
00:02:37,330 --> 00:02:39,340
为了获得巨大的综合性能
aggregate performance be able to harness

62
00:02:39,340 --> 00:02:43,090
要能利用数百台机器的资源 来完成大量工作
the resources of hundreds of machines in

63
00:02:43,090 --> 00:02:44,620
要能利用数百台机器的资源 来完成大量工作
order to get a huge amount of work done

64
00:02:44,620 --> 00:02:48,000
因此，性能问题就成为了最初的诉求
so the sort of starting point is often

65
00:02:48,000 --> 00:02:54,430
因此 性能问题就成为了最初的诉求
performance and you know if you start

66
00:02:54,430 --> 00:02:57,010
因此有个很自然的想法
there a natural next thought is well

67
00:02:57,010 --> 00:02:59,019
将数据进行分割 放到大量服务器上
we're gonna split our data over a huge

68
00:02:59,019 --> 00:03:00,640
将数据进行分割 放到大量服务器上
number of servers in order to be able to

69
00:03:00,640 --> 00:03:04,420
从而能够并行地 从多台服务器读取数据
read many servers in parallel so we're

70
00:03:04,420 --> 00:03:05,769
通常 我们把这种方式称为 分片(sharding)
gonna get and that's often called

71
00:03:05,769 --> 00:03:11,160
如果你将数据分片到成百上千台服务器上
sharding if you shard over many servers

72
00:03:11,160 --> 00:03:13,600
如果你将数据分片到成百上千台服务器上
hundreds or thousands of servers you're

73
00:03:13,600 --> 00:03:15,970
你会发现出错成了常态
just gonna see constant faults right if

74
00:03:15,970 --> 00:03:17,140
在上千台服务器中 总有那么几台会宕机
you have thousands of servers there's

75
00:03:17,140 --> 00:03:20,680
在上千台服务器中 总有那么几台会宕机
just always gonna be one down so we

76
00:03:20,680 --> 00:03:25,540
所以每天 每小时都在发生错误
the faults are just every day every hour

77
00:03:25,540 --> 00:03:27,250
所以 我们需要自动化的方法去纠正它
occurrences and we need automatic

78
00:03:27,250 --> 00:03:29,350
你不可能靠人工去发现并纠正这些错误
we can't have human involved and fixing

79
00:03:29,350 --> 00:03:31,890
所以 得需要有一个自动化的 容错(fault-tolerant, ft)系统
this fault we need automatic

80
00:03:31,890 --> 00:03:38,290
所以 得需要有一个自动化的 容错(fault-tolerant, ft)系统
fault-tolerant systems so that leads to

81
00:03:38,290 --> 00:03:43,090
这就引出 容错 这个话题
fault tolerance the among the most

82
00:03:43,090 --> 00:03:44,920
实现容错最有用的一种方法是使用复制
powerful ways to get fault tolerance is

83
00:03:44,920 --> 00:03:46,630
实现容错最有用的一种方法是使用复制
with replication just keep two or three

84
00:03:46,630 --> 00:03:48,190
只需保留两三个或更多数量数据副本
or whatever copies of data one of them

85
00:03:48,190 --> 00:03:52,390
只要其中有一个失败，你就可以使用另一个
fails you can use another one so we want

86
00:03:52,390 --> 00:03:56,010
所以，如果想要容错能力，就得有 复制（replication）
to have tolerance that leads to

87
00:03:56,010 --> 00:04:03,100
如果有复制 两份数据的副本
replication if you have replication two

88
00:04:03,100 --> 00:04:05,470
如果有复制 两份数据的副本
copies the data then you know for sure

89
00:04:05,470 --> 00:04:07,329
可以确定的是 如果你不小心 它们就会不一致
if you're not careful they're gonna get

90
00:04:07,329 --> 00:04:09,010
所以如果你有 2 个数据副本（replicas）
out of sync and so what you thought was

91
00:04:09,010 --> 00:04:10,750
你就能使用其中之一替换另一个 进行容错
two replicas of the data where you could

92
00:04:10,750 --> 00:04:12,549
你就能使用其中之一替换另一个 进行容错
use either one interchangeably to

93
00:04:12,549 --> 00:04:14,170
你就能使用其中之一替换另一个 进行容错
tolerate faults if you're not careful

94
00:04:14,170 --> 00:04:15,670
如果你不小心弄出两个几乎不同的 replicas
what you end up with is two almost

95
00:04:15,670 --> 00:04:18,640
如果你不小心弄出两个几乎不同的 replicas
identical replicas of the data that's

96
00:04:18,640 --> 00:04:20,289
严格来讲它们就算不上 replicas
like not exactly replicas at all and

97
00:04:20,289 --> 00:04:22,180
那么 你获得的数据 将取决于与向哪个服务器请求
what you get back depends on which one

98
00:04:22,180 --> 00:04:24,039
那么 你获得的数据 将取决于与向哪个服务器请求
you talk to so that's starting to maybe

99
00:04:24,039 --> 00:04:25,240
这样的话 应用程序用起来就有点棘手
look a little bit

100
00:04:25,240 --> 00:04:28,420
这样的话 应用程序用起来就有点棘手
tricky for applications to use so if we

101
00:04:28,420 --> 00:04:34,330
所以如果采用复制 就可能遇到奇怪的不一致问题
have replication we risk weird

102
00:04:34,330 --> 00:04:41,800
所以如果采用复制 就可能遇到奇怪的不一致问题
inconsistencies of course clever design

103
00:04:41,800 --> 00:04:45,400
当然 有聪明的设计 可以让你避免不一致
you can get rid of inconsistency and

104
00:04:45,400 --> 00:04:47,680
并使数据看起来是行为良好的
make the data look very well-behaved but

105
00:04:47,680 --> 00:04:49,450
但这种设计的代价是 你得做更多额外的工作
if you do that it almost always requires

106
00:04:49,450 --> 00:04:51,210
但这种设计的代价是 你得做更多额外的工作
extra work and extra sort of chitchat

107
00:04:51,210 --> 00:04:53,140
以及网络中所有不同服务器和客户端之间的额外通信
between all the different servers and

108
00:04:53,140 --> 00:04:54,610
以及网络中所有不同服务器和客户端之间的额外通信
clients in the network that reduces

109
00:04:54,610 --> 00:04:58,470
这样性能也就变低了
performance so if you want consistency

110
00:04:59,550 --> 00:05:09,190
因此，如果要保持一致性，就需要付出代价
you pay for with low performance I which

111
00:05:09,190 --> 00:05:11,740
当然这也违背了我们的初衷
is of course not what we originally

112
00:05:11,740 --> 00:05:13,420
当然这是绝对的
hoping for of course this is an absolute

113
00:05:13,420 --> 00:05:14,650
你可以构建 高性能的系统
you can build very high performance

114
00:05:14,650 --> 00:05:16,990
在设计这些系统的时候 这些事情你都需要去面对
systems but nevertheless there's this

115
00:05:16,990 --> 00:05:19,480
在设计这些系统的时候 这些事情你都需要去面对
sort of inevitable way that the design

116
00:05:19,480 --> 00:05:21,370
在设计这些系统的时候 这些事情你都需要去面对
of these systems play out and it results

117
00:05:21,370 --> 00:05:24,670
所以这就会导致你需要在
in a tension between the original goals

118
00:05:24,670 --> 00:05:26,920
性能目标和实现良好的一致性之间做权衡
of performance and the sort of

119
00:05:26,920 --> 00:05:29,020
性能目标和实现良好的一致性之间做权衡
realization that if you want good

120
00:05:29,020 --> 00:05:31,720
并因此付出一些代价
consistency you're gonna pay for it and

121
00:05:31,720 --> 00:05:33,730
如果你不想付出代价 那你就得接受系统的异常行为
if you don't want to pay for it then you

122
00:05:33,730 --> 00:05:35,830
如果你不想付出代价 那你就得接受系统的异常行为
have to suffer with sort of anomalous

123
00:05:35,830 --> 00:05:37,930
我把这些东西写在这里
behavior sometimes I'm putting this up

124
00:05:37,930 --> 00:05:39,840
因为在未来许多系统里 你都能经常见到这些东西
because we're gonna see this this loop

125
00:05:39,840 --> 00:05:42,310
因为在未来许多系统里 你都能经常见到这些东西
many times for many of the systems we

126
00:05:42,310 --> 00:05:45,580
有很多人都不太愿意在良好的一致性（强一致性）上面花费精力
look we look at people are rarely

127
00:05:45,580 --> 00:05:48,070
有很多人都不太愿意在良好的一致性（强一致性）上面花费精力
willing to or happy about paying the

128
00:05:48,070 --> 00:05:52,930
有很多人都不太愿意在良好的一致性（强一致性）上面花费精力
full cost of very good consistency ok so

129
00:05:52,930 --> 00:05:57,520
在接下来的课程里我将会讨论更多关于良好一致性的话题
you know with brought a consistency I'll

130
00:05:57,520 --> 00:06:02,050
在接下来的课程里我将会讨论更多关于良好一致性的话题
talk more later in the course about more

131
00:06:02,050 --> 00:06:04,000
在接下来的课程里我将会讨论更多关于良好一致性的话题
exactly what I mean by good consistency

132
00:06:04,000 --> 00:06:07,000
你可以把“强一致性”(strong consistency)
but you can think of strong consistency

133
00:06:07,000 --> 00:06:09,280
或是“良好一致性”的系统想象成就像是一台单机 server 一样
or good consistency as being we want to

134
00:06:09,280 --> 00:06:11,410
或是“良好一致性”的系统想象成就像是一台单机 server 一样
build a system whose behavior to

135
00:06:11,410 --> 00:06:13,930
应用程序或是客户端和它通信，感觉就像是和一台 server 通信
applications or clients looks just like

136
00:06:13,930 --> 00:06:15,610
应用程序或是客户端和它通信，感觉就像是和一台 server 通信
you'd expect from talking to a single

137
00:06:15,610 --> 00:06:18,760
应用程序或是客户端和它通信，感觉就像是和一台 server 通信
server all right we're gonna build you

138
00:06:18,760 --> 00:06:20,260
在数百台机器上的构建的理想的强一致模型的系统
know systems out of hundreds of machines

139
00:06:20,260 --> 00:06:23,170
在数百台机器上的构建的理想的强一致模型的系统
but a kind of ideal strong consistency

140
00:06:23,170 --> 00:06:25,000
在同一时刻你只能请求到一台 server 上的一个数据副本
model would be what you'd get if there

141
00:06:25,000 --> 00:06:26,560
在同一时刻你只能请求到一台 server 上的一个数据副本
was just one server with one copy of the

142
00:06:26,560 --> 00:06:31,810
在同一时刻你只能请求到一台 server 上的一个数据副本
data doing one thing at a time so this

143
00:06:31,810 --> 00:06:34,349
这是思考强一致的一种最直觉的方式
is kind of a strong

144
00:06:34,349 --> 00:06:41,169
这是思考强一致的一种最直觉的方式
consistency kind of intuitive way to

145
00:06:41,169 --> 00:06:42,789
这是思考强一致的一种最直觉的方式
think about strong consistency so you

146
00:06:42,789 --> 00:06:45,490
你可以想象你有一台服务器
might think you have one server we'll

147
00:06:45,490 --> 00:06:47,020
这里假设它是单线程服务
assume that's a single-threaded server

148
00:06:47,020 --> 00:06:49,210
在同一时刻它处理来自多个客户端的请求
and that it processes requests from

149
00:06:49,210 --> 00:06:50,919
在同一时刻它处理来自多个客户端的请求
clients one at a time and that's

150
00:06:50,919 --> 00:06:52,569
这很重要，因为有大量客户端可能会同时并发的请求 server
important because there may be lots of

151
00:06:52,569 --> 00:06:55,509
这很重要，因为有大量客户端可能会同时并发的请求 server
clients sending concurrently requests

152
00:06:55,509 --> 00:06:57,370
server 看到这些请求后
into the server and see some current

153
00:06:57,370 --> 00:06:59,020
首先会从中挑选一个出来（TODO）
requests it picks one or the other to go

154
00:06:59,020 --> 00:07:00,729
首先会从中挑选一个出来
first and excute that request to

155
00:07:00,729 --> 00:07:04,090
处理请求结束后然后回复
completion then excute the nets so for

156
00:07:04,090 --> 00:07:06,099
对于存储服务来说 你知道它会有一块磁盘
storage servers or you know the server's

157
00:07:06,099 --> 00:07:07,629
对于存储服务来说 你知道它会有一块磁盘
got a disk on it and what it means to

158
00:07:07,629 --> 00:07:10,060
这意味着处理一条请求可能会写入一条 item
process a request is it's a write

159
00:07:10,060 --> 00:07:12,610
这意味着处理一条请求可能会写入一条 item
request you know which might be writing

160
00:07:12,610 --> 00:07:14,710
也可能是给某个 item 做自增
an item or may be increment and I mean

161
00:07:14,710 --> 00:07:17,979
这里自增一个 item 是指它是否是一个可修改操作
incrementing an item if it's a mutation

162
00:07:17,979 --> 00:07:21,069
我有一些数据记录在这个表中
then we're gonna go and we have some

163
00:07:21,069 --> 00:07:23,680
它可能是通过 key-value 进行索引的数据
table of data and you know maybe index

164
00:07:23,680 --> 00:07:25,240
它可能是通过 key-value 进行索引的数据
by keys and values and we're gonna

165
00:07:25,240 --> 00:07:27,039
然后我们打算更新这个表
update this table and if the request

166
00:07:27,039 --> 00:07:28,240
如果一条请求进来读取它
comes in and to read we're just gonna

167
00:07:28,240 --> 00:07:30,099
只需要把写入的数据拉出来就行了
you know pull the write data out of the

168
00:07:30,099 --> 00:07:36,759
这里有一条法则
table one of the rules here that sort of

169
00:07:36,759 --> 00:07:39,580
要想使得 server 行为良好
makes this well-behaved is that each is

170
00:07:39,580 --> 00:07:41,740
那么 server 必须以一种简单的模型去运行
that the server really does execute in

171
00:07:41,740 --> 00:07:44,710
即同一时刻，只处理一条请求
our simplified model excuse to request

172
00:07:44,710 --> 00:07:48,129
即同一时刻，只处理一条请求
one at a time and that requests see data

173
00:07:48,129 --> 00:07:49,990
任何请求看到的数据
that reflects all the previous

174
00:07:49,990 --> 00:07:51,819
都能反应出在这之前所有的有序的操作
operations in order so if a sequence of

175
00:07:51,819 --> 00:07:53,560
所以 如果有一系列的写操作请求同时到来
writes come in and the server process

176
00:07:53,560 --> 00:07:55,360
server 就会有序的处理它们
them in some order then when you read

177
00:07:55,360 --> 00:07:58,060
当你读取的时候 你就能读取到你期望的值
you see the sort of you know value you

178
00:07:58,060 --> 00:08:00,009
当你读取的时候 你就能读取到你期望的值
would expect if those writes that

179
00:08:00,009 --> 00:08:05,169
当你读取的时候 你就能读取到你期望的值
occurred one at a time the behavior this

180
00:08:05,169 --> 00:08:07,029
这个行为仍然没那么简单
is still not completely straightforward

181
00:08:07,029 --> 00:08:09,659
你不得不花上几秒的时间去考虑一些事情
there's some you know there's some

182
00:08:09,659 --> 00:08:11,919
你不得不花上几秒的时间去考虑一些事情
things that you have to spend at least a

183
00:08:11,919 --> 00:08:13,629
你不得不花上几秒的时间去考虑一些事情
second thinking about so for example if

184
00:08:13,629 --> 00:08:19,539
比如我们有大量的客户端
we have a bunch of clients and client

185
00:08:19,539 --> 00:08:25,180
客户端 C1 发起写请求写 x 并把它的值设置成 1
one issues a write of value X and wants

186
00:08:25,180 --> 00:08:27,460
客户端 C1 发起写请求写 x 并把它的值设置成 1
it to set it to one and at the same time

187
00:08:27,460 --> 00:08:30,460
在同一时刻 客户端 C2 发起请求也写 x
client two issues the right of the same

188
00:08:30,460 --> 00:08:32,159
但是它想把值设置成一个不同的值(2)
value but wants to set it to a different

189
00:08:32,159 --> 00:08:34,360
但是它想把值设置成一个不同的值(2)
the same key but wants to set it to a

190
00:08:34,360 --> 00:08:35,860
但是它想把值设置成一个不同的值(2)
different value right

191
00:08:35,860 --> 00:08:38,409
现在有别的事情发生了
something happens let's say client three

192
00:08:38,409 --> 00:08:42,490
客户端 C3 请求读并得到一个结果
reads and get some result or client

193
00:08:42,490 --> 00:08:44,020
或者说 C3 在写请求结束后读取会得到一个结果
three after these writes complete reads

194
00:08:44,020 --> 00:08:47,220
或者说 C3 在写请求结束后读取会得到一个结果
get some result client four

195
00:08:47,220 --> 00:08:50,290
客户端 C4 读取 x 也得到一个结果
reads X and get some also gets a result

196
00:08:50,290 --> 00:08:51,999
那么这两个客户端看到的结果是多少？
so what results should the two clients

197
00:08:51,999 --> 00:09:00,959
(提问)
see yeah

198
00:09:04,700 --> 00:09:07,200
很好，这个问题不错
well that's a good question so these

199
00:09:07,200 --> 00:09:09,060
这里的客户端 C1 和 C2
what I'm assuming here is that client

200
00:09:09,060 --> 00:09:10,770
我假设它们是在同时发起的请求
one and client two launch these requests at

201
00:09:10,770 --> 00:09:12,720
如果监控网络的话 我们能看到 2 个请求同时到达服务器
the same time so if we were monitoring

202
00:09:12,720 --> 00:09:14,190
如果监控网络的话 我们能看到 2 个请求同时到达服务器
the network we'd see two requests

203
00:09:14,190 --> 00:09:16,500
如果监控网络的话 我们能看到 2 个请求同时到达服务器
heading to the server at the same time

204
00:09:16,500 --> 00:09:19,710
接下来在某个时刻后 服务器会回复它们
and then sometime later the server would

205
00:09:19,710 --> 00:09:20,520
接下来在某个时刻后 服务器会回复它们
respond to them

206
00:09:20,520 --> 00:09:23,790
这里不足以判断客户端是否收到了第一条请求的回复
so there's actually not enough here to

207
00:09:23,790 --> 00:09:26,070
这里不足以判断客户端是否收到了第一条请求的回复
be able to say whether the client would

208
00:09:26,070 --> 00:09:28,530
不足以判断第一条请求是否被处理
receive would process the first request

209
00:09:28,530 --> 00:09:30,780
也不足以判断服务器是以何种方式处理的它们
first which order there's not enough

210
00:09:30,780 --> 00:09:32,880
也不足以判断服务器是以何种方式处理的它们
here to tell which order the server

211
00:09:32,880 --> 00:09:35,460
也不足以判断服务器是以何种方式处理的它们
processes them in and of course if it

212
00:09:35,460 --> 00:09:38,580
当然了，如果它先处理这个请求
processes this request first then that

213
00:09:38,580 --> 00:09:41,760
然后就意味着它会处理这边这条请求 把值设置成 2
means or it processes the right with

214
00:09:41,760 --> 00:09:43,800
然后就意味着它会处理这边这条请求 把值设置成 2
value to second and that means that

215
00:09:43,800 --> 00:09:46,350
接下来后续的读将看到 2
subsequent reads have to see two where is

216
00:09:46,350 --> 00:09:48,030
如果 server 先处理这条请求
it the server happened to process this

217
00:09:48,030 --> 00:09:50,250
然后再处理这条请求
request first and this one's second that

218
00:09:50,250 --> 00:09:52,020
这意味着最终的结果是 1
means the resulting value better be one

219
00:09:52,020 --> 00:09:54,060
然后这两条读请求就会看到 1
and these these two requests and see one

220
00:09:54,060 --> 00:09:56,670
提出这个问题的目的是想说明
so I'm just putting this up to sort

221
00:09:56,670 --> 00:09:58,950
即使是在一个非常简单的系统里也会存在模棱两可的结果
of illustrate that even in a simple

222
00:09:58,950 --> 00:10:01,230
即使是在一个非常简单的系统里也会存在模棱两可的结果
system there's ambiguity you can't

223
00:10:01,230 --> 00:10:04,020
你没办法判断出服务器到底会先处理哪条请求
necessarily tell from trace of what went

224
00:10:04,020 --> 00:10:05,190
也无法判断哪条回复会被先发出去
into the server or what should come out

225
00:10:05,190 --> 00:10:08,820
你能做到的只是判断
all of you can tell is that some set of

226
00:10:08,820 --> 00:10:11,250
在可能的执行上 产生的结果是一致还是不一致的
results is consistent or not consistent

227
00:10:11,250 --> 00:10:13,470
在可能的执行上 产生的结果是一致还是不一致的
with a possible execution so certainly

228
00:10:13,470 --> 00:10:17,850
所以我们肯定能看到有一些完全错误的结果
there's some completely wrong results we

229
00:10:17,850 --> 00:10:21,060
所以我们肯定能看到有一些完全错误的结果
can see go by it you know if client 3

230
00:10:21,060 --> 00:10:24,030
如果客户端 C3 看到了 2
sees a 2 then client 4 I bet had better

231
00:10:24,030 --> 00:10:27,210
客户端 C4 最好也要能看到 2
see a two also because our model is

232
00:10:27,210 --> 00:10:29,040
因为我们的模型保证第二次写之后是没有问题的
well after the second write you know

233
00:10:29,040 --> 00:10:30,750
客户端 C3 能看到 2 意味着这条请求一定是第 2 次执行
client three sees it two that means

234
00:10:30,750 --> 00:10:33,870
客户端 C3 能看到 2 意味着这条请求一定是第 2 次执行
this write must have been second and it

235
00:10:33,870 --> 00:10:35,700
最好是这样 当客户端 C4 能读取到 2 时
still had better be it still has to have

236
00:10:35,700 --> 00:10:37,620
它也必须是第二次写入
been the second write when client 4 goes 2

237
00:10:37,620 --> 00:10:41,220
希望这些东西足够直白
to read the data so hopefully all this is

238
00:10:41,220 --> 00:10:43,410
希望这些东西足够简单 且如预期那样
just completely straightforward and just

239
00:10:43,410 --> 00:10:47,790
因为它就是强一致性的直观模型
as expected because it's it's supposed

240
00:10:47,790 --> 00:10:49,200
因为它就是强一致性的直观模型
to be the intuitive model of strong

241
00:10:49,200 --> 00:10:53,190
ok 这有什么问题呢？
consistency ok and so the problem with

242
00:10:53,190 --> 00:10:54,300
问题在于单个服务器的容错能力很差
this of course is that a single server

243
00:10:54,300 --> 00:10:56,370
问题在于单个服务器的容错能力很差
has poor fault tolerance right if it

244
00:10:56,370 --> 00:10:57,840
如果它 crash 了 或是磁盘坏了
crashes or it's disk dies or something

245
00:10:57,840 --> 00:11:00,870
那就什么东西都留不下来了
we're left with nothing and so in the

246
00:11:00,870 --> 00:11:02,520
所以在现实世界中的分布式系统
real world of distributed systems we

247
00:11:02,520 --> 00:11:05,430
我们实际上都会构建一个复制系统
actually build replicated systems so and

248
00:11:05,430 --> 00:11:06,930
但是 当我们有了第二份数据副本时
that's where all the problems start

249
00:11:06,930 --> 00:11:08,220
但是 当我们有了第二份数据副本时
leaking in is when we have a second

250
00:11:08,220 --> 00:11:12,060
这成为了所有问题的发源地
copy data so here is what must be

251
00:11:12,060 --> 00:11:16,180
这里有一个最糟糕的副本设计方案
close to the worst replication design

252
00:11:16,180 --> 00:11:19,220
我写这个是为了警告你
and I'm doing this to warn you of the

253
00:11:19,220 --> 00:11:20,810
这个问题在 GFS 中你也会发现
problems that we will then be looking

254
00:11:20,810 --> 00:11:23,960
好 这里应该写 bad replication design
for in GFS all right so here's a bad

255
00:11:23,960 --> 00:11:30,380
假设现在我们有 2 台 server
replication design we're gonna have two

256
00:11:30,380 --> 00:11:32,630
每台都有一份完整的数据副本
servers now each with a complete copy of

257
00:11:32,630 --> 00:11:38,510
然后在磁盘上 他们都有这样一个 key-value 表格
the data and so on disks that are both

258
00:11:38,510 --> 00:11:40,730
然后在磁盘上 他们都有这样一个 key-value 表格
gonna have this this table of keys and

259
00:11:40,730 --> 00:11:44,810
直观上 当然了 我们希望让这两张表完全一致
values the intuition of course is that

260
00:11:44,810 --> 00:11:47,090
直观上 当然了 我们希望让这两张表完全一致
we want to keep these tables we hope to

261
00:11:47,090 --> 00:11:49,880
直观上 当然了 我们希望让这两张表完全一致
keep these tables identical so that if

262
00:11:49,880 --> 00:11:51,650
所以 如果其中一台 server 失败了
one server fails we can read or write

263
00:11:51,650 --> 00:11:53,720
我们可以读写另一台服务器
from the other server and so that means

264
00:11:53,720 --> 00:11:55,490
这意味着 每一个写操作都必须在所有的 server 处理
that somehow every write must be

265
00:11:55,490 --> 00:11:59,210
这意味着 每一个写操作都必须在所有的 server 处理
processed by both servers and reads have

266
00:11:59,210 --> 00:12:00,890
另外 read 只能在单台 server 处理 否则就无法容错了
to be able to be processed by a single

267
00:12:00,890 --> 00:12:02,570
另外 read 只能在单台 server 处理 否则就无法容错了
server otherwise it's not fault tolerant

268
00:12:02,570 --> 00:12:04,280
因为 如果 read 必须同时和 2 台服务器打交道
all right if reads have to consult both

269
00:12:04,280 --> 00:12:07,940
就无法在失去其中一台 server 的情况下幸免
and we can't survive the loss of one of

270
00:12:07,940 --> 00:12:13,160
所以问题就来了
the servers okay so the problem is gonna

271
00:12:13,160 --> 00:12:17,030
假设客户端 C1 和 C2 同时想执行这些写操作
come up well I suppose we have client 1

272
00:12:17,030 --> 00:12:19,190
假设客户端 C1 和 C2 同时想执行这些写操作
and client 2 and they both want to do

273
00:12:19,190 --> 00:12:20,570
其中一个写 1 另一个写 2
these write say one of them gonna write

274
00:12:20,570 --> 00:12:22,250
其中一个写 1 另一个写 2
one and the other is going to write two

275
00:12:22,250 --> 00:12:25,790
所以客户端 C1 需要将请求 Wx1 发送给两台服务器
so client 1 is gonna launch it's write

276
00:12:25,790 --> 00:12:29,270
因为我们就是想更新这 2 台 server
x1 to both because we want to update both

277
00:12:29,270 --> 00:12:32,600
客户端 C2 将请求 Wx2 发给这 2 台 server
of them and clent 2 is gonna launch it's

278
00:12:32,600 --> 00:12:41,800
所以这里就导致出错了
write X so what's gonna go wrong here

279
00:12:41,800 --> 00:12:46,280
(提问) 是的 我们没有做任何事情来保障
yeah yeah we haven't done anything here

280
00:12:46,280 --> 00:12:48,410
2 台 server 以相同的顺序处理这 2 个请求
to ensure that the two servers process

281
00:12:48,410 --> 00:12:51,590
2 台 server 以相同的顺序处理这 2 个请求
the two requests in the same order right

282
00:12:51,590 --> 00:12:53,930
这个设计真的不怎么样
that's a bad design

283
00:12:53,930 --> 00:12:57,800
如果 S1 先处理 C1 的请求
so if server 1 processes client ones

284
00:12:57,800 --> 00:13:01,100
处理结束后这里这个值变成 1
request first it'll end up it'll start

285
00:13:01,100 --> 00:13:02,600
处理结束后这里这个值变成 1
with a value of 1 and then it'll see

286
00:13:02,600 --> 00:13:04,610
接下来 C2 请求过来 覆盖了这个值变成了 2
client twos request and overwrite that

287
00:13:04,610 --> 00:13:07,610
如果 S2 在接收网络报文的时候
with 2 if server 2 just happens to

288
00:13:07,610 --> 00:13:09,350
刚好是不同的顺序 先收到 C2 的请求先把值设置成 2
receive the packets over the network in

289
00:13:09,350 --> 00:13:11,020
刚好是不同的顺序 先收到 C2 的请求先把值设置成 2
a different order it's going to execute

290
00:13:11,020 --> 00:13:13,310
刚好是不同的顺序 先收到 C2 的请求先把值设置成 2
client 2's requests and set the value to

291
00:13:13,310 --> 00:13:15,350
然后收到 C1 的请求 把值设置成了 1
2 and then then it will see client ones

292
00:13:15,350 --> 00:13:18,140
然后收到 C1 的请求 把值设置成了 1
request set the value to 1 and now what

293
00:13:18,140 --> 00:13:20,450
那么现在 后来的客户端读取的话
a client a later reading client sees you

294
00:13:20,450 --> 00:13:22,760
你应该很清楚 如果碰巧 C3 请求了这台 server
know if client 3 happens to reach from

295
00:13:22,760 --> 00:13:25,520
你应该很清楚 如果碰巧 C3 请求了这台 server
this server and client for happens to

296
00:13:25,520 --> 00:13:26,720
而 C4 碰巧访问了另一台 server
reach from the other server then we get

297
00:13:26,720 --> 00:13:28,610
于是我们就陷入了一种可怕的场景
into this terrible situation where

298
00:13:28,610 --> 00:13:30,320
他们读取到的值不同
they're gonna read different values even

299
00:13:30,320 --> 00:13:33,410
尽管我们对正确服务的直观模型表明
though our intuitive model of a correct

300
00:13:33,410 --> 00:13:35,990
在随后的 2 次读他们都产生了相同的值
service says they both subsequent reads

301
00:13:35,990 --> 00:13:39,589
在随后的 2 次读他们都产生了相同的值
have to yeild the same value and this can

302
00:13:39,589 --> 00:13:41,930
这种问题仍然以另一种方式暴露出来
arise in other ways you know suppose we

303
00:13:41,930 --> 00:13:43,579
假设客户端尝试
try to fix this by making the clients

304
00:13:43,579 --> 00:13:45,920
只从 S1 或是 S2 读取来修复这个问题
always read from server one if it's up

305
00:13:45,920 --> 00:13:48,829
只从 S1 或是 S2 读取来修复这个问题
and otherwise server two if we do that

306
00:13:48,829 --> 00:13:51,350
如果我们这样做了 然后也发生了这种情况
then if this situation happened and for

307
00:13:51,350 --> 00:13:53,089
过了一会儿 这 2 个客户端都去读这个 server
a while，yeah both everybody reads might

308
00:13:53,089 --> 00:13:55,279
这 2 个客户端或许都能看见值 2
see client might see value two but a

309
00:13:55,279 --> 00:13:57,649
但是 S1 突然宕机了
server one suddenly fails then even

310
00:13:57,649 --> 00:14:00,290
x 读取到的值从 2 变成了 1 尽管没有写操作 
though there was no write suddenly the

311
00:14:00,290 --> 00:14:02,050
x 读取到的值从 2 变成了 1 尽管没有写操作 
value for X we'll switch from 2 to 1

312
00:14:02,050 --> 00:14:04,850
因为 S1 宕机了 所有的客户端都会切换到 S2
because if server 1 died it's all the

313
00:14:04,850 --> 00:14:07,130
因为 S1 宕机了 所有的客户端都会切换到 S2
clients will switch to server 2 not but just

314
00:14:07,130 --> 00:14:09,079
不只是这种数据产生的离奇变化
this mysterious change in the data that

315
00:14:09,079 --> 00:14:11,570
它不和任何写对应
doesn't correspond to any write which is

316
00:14:11,570 --> 00:14:13,190
这种事情完全不应该在服务中发生
also totally not something that could

317
00:14:13,190 --> 00:14:15,680
这种事情完全不应该在这种简单的服务模型中发生
have happened in this service simple

318
00:14:15,680 --> 00:14:23,329
这种事情完全不应该在这种简单的服务模型中发生
server model all right so of course this

319
00:14:23,329 --> 00:14:25,940
当然了 这个问题可以被修复
can be fixed the fix requires more

320
00:14:25,940 --> 00:14:28,220
修复它需要 server 之间进行更多的通信
communication usually between the

321
00:14:28,220 --> 00:14:33,529
某些地方也会变得更加复杂
servers or somewhere more complexity and

322
00:14:33,529 --> 00:14:36,649
因为想获得强一致性 不可避免的需要更多开销
because of the cost of inevitable cost

323
00:14:36,649 --> 00:14:37,820
因为想获得强一致性 不可避免的需要更多开销
to the complexity to get strong

324
00:14:37,820 --> 00:14:41,180
有大量的解决方案可以获得更好的一致性
consistency there's a whole range of

325
00:14:41,180 --> 00:14:43,610
有大量的解决方案可以获得更好的一致性
different solutions to get better

326
00:14:43,610 --> 00:14:45,769
也有大量解决方案可以获得让人感觉还能接受的一致性
consistency and a whole range of what

327
00:14:45,769 --> 00:14:48,350
也有大量解决方案可以获得让人感觉还能接受的一致性
people feel is an acceptable level of

328
00:14:48,350 --> 00:14:52,250
也有大量解决方案可以获得让人感觉还能接受的一致性
consistency in an acceptable sort of a

329
00:14:52,250 --> 00:14:54,890
就算出现一些小瑕疵也还能接受
set of anomalous behaviors that might be

330
00:14:54,890 --> 00:14:57,560
好 关于这个灾难性的模型 还有什么问题吗？
revealed all right any questions about

331
00:14:57,560 --> 00:15:03,910
好 关于这个灾难性的模型 还有什么问题吗？
this disastrous model here

332
00:15:04,649 --> 00:15:07,779
okay 这些就是关于 GFS 的讨论
okay that's what you're talking about

333
00:15:07,779 --> 00:15:13,209
一些关于实现 GFS 的思考 以及修复这些问题
GFS a lot of thought about doing GFS was

334
00:15:13,209 --> 00:15:17,079
一些关于实现 GFS 的思考 以及修复这些问题
doing is fixing this they had better but

335
00:15:17,079 --> 00:15:21,790
他们做的挺好 但是还不够完美
not perfect behavior okay so where GFS

336
00:15:21,790 --> 00:15:24,179
GFS 是在 2003 年提出的 已经过去相当长一段时间
came from in 2003 quite a while ago

337
00:15:24,179 --> 00:15:27,730
那时候 web 发展到相当规模了
actually at that time the the web you

338
00:15:27,730 --> 00:15:29,379
那时候 web 发展到相当规模了 人们也在建立大型网站
know was certainly starting to be a very

339
00:15:29,379 --> 00:15:31,569
那时候 web 发展到相当规模了 人们也在建立大型网站
big deal and people were building big

340
00:15:31,569 --> 00:15:35,439
此外 分布式系统领域也有了几十年的研究
websites in addition there had been

341
00:15:35,439 --> 00:15:37,540
此外 分布式系统领域也有了几十年的研究
decades of research into distributed

342
00:15:37,540 --> 00:15:39,009
人们知道至少在学术领域
systems and people sort of knew at least

343
00:15:39,009 --> 00:15:40,509
如何构建各种类型的 高度并行化的且具备容错的系统
at the academic level how to build all

344
00:15:40,509 --> 00:15:43,119
如何构建各种类型的 高度并行化的且具备容错的系统
kinds of highly parallel fault tolerant

345
00:15:43,119 --> 00:15:44,739
但是学术上的点子 很少有能应用在工业领域
whatever systems but there been very

346
00:15:44,739 --> 00:15:49,589
但是学术上的点子 很少有能应用在工业领域
little use of academic ideas in industry

347
00:15:49,589 --> 00:15:52,239
但从这篇论文发表之后
but starting at around the time this

348
00:15:52,239 --> 00:15:54,759
像 Google 这样的大型网站才开始真正建立严格意义上分布式系统
paper was published big websites like

349
00:15:54,759 --> 00:15:57,399
像 Google 这样的大型网站才开始真正建立严格意义上分布式系统
Google started to actually build serious

350
00:15:57,399 --> 00:16:01,569
这件事让人热血沸腾 其中就包括我
distributed systems and it was like very

351
00:16:01,569 --> 00:16:03,699
这件事让人热血沸腾 其中就包括我
exciting for people like me who were on

352
00:16:03,699 --> 00:16:06,879
作为学术界的一份子
academic side of this to see see real

353
00:16:06,879 --> 00:16:10,119
我切实体会到了所有想法在工业界得以实现
uses of these ideas where Google was

354
00:16:10,119 --> 00:16:11,769
在 Google, 有海量的数据
coming from was you know they had some

355
00:16:11,769 --> 00:16:14,470
这些数据多到单个磁盘远远无法存储
vast vast data sets far larger than

356
00:16:14,470 --> 00:16:16,360
这些数据多到单个磁盘远远无法存储
could be stored in a single disk like an

357
00:16:16,360 --> 00:16:20,769
就比如说整个互联网抓取的网页副本
entire crawl copy of the web or a little

358
00:16:20,769 --> 00:16:22,119
或者在这篇文章后边一点有个巨大的 YouTube 视频
bit after this paper they had giant

359
00:16:22,119 --> 00:16:25,480
他们就会有一份比如中间文件的东西 用来建立索引用于搜索
YouTube videos they had things like the

360
00:16:25,480 --> 00:16:27,669
他们就会有一份比如中间文件的东西 用来建立索引用于搜索
intermedia files for building a search

361
00:16:27,669 --> 00:16:28,299
他们就会有一份比如中间文件的东西 用来建立索引用于搜索
index

362
00:16:28,299 --> 00:16:30,790
他们的 web 服务器显然会有大量的日志文件
they also apparently kept enormous log

363
00:16:30,790 --> 00:16:32,679
他们的 web 服务器显然会有大量的日志文件
files from all their web servers so they

364
00:16:32,679 --> 00:16:34,029
以便用作未来分析
could later analyze them so they had

365
00:16:34,029 --> 00:16:36,910
他们有大量的数据集 并且使用大量的磁盘来存储他们
some big big data sets they used both to

366
00:16:36,910 --> 00:16:39,339
他们有大量的数据集 并且使用大量的磁盘来存储他们
store them and many many disks to store

367
00:16:39,339 --> 00:16:41,139
借助 MapReduce 这样的工具可以快速的处理这些数据
them and they needed to be able to

368
00:16:41,139 --> 00:16:42,399
借助 MapReduce 这样的工具可以快速的处理这些数据
process them quickly with things like

369
00:16:42,399 --> 00:16:44,709
因此他们需要能高度并行化的访问海量数据
MapReduce so they needed high speed

370
00:16:44,709 --> 00:16:47,529
因此他们需要能高度并行化的访问海量数据
parallel access to these vast amounts of

371
00:16:47,529 --> 00:16:51,819
okay 所以他们的目标是
data okay so what they were looking for

372
00:16:51,819 --> 00:16:53,669
这个存储系统需要 大容量 速度快
one goal was just that the thing be big

373
00:16:53,669 --> 00:17:00,009
他们还需要一个文件系统 从某种意义上来说
and fast they also wanted a file system

374
00:17:00,009 --> 00:17:02,470
这个文件系统必须是 Global 的（覆盖在整个数据中心上）
that was sort of global in the sense

375
00:17:02,470 --> 00:17:04,148
从某种意义上来说 各种不同的应用程序都能从中读取数据
that many different applications could

376
00:17:04,148 --> 00:17:06,490
有一种建立大型存储系统的方法
get at it one way to build a big storage

377
00:17:06,490 --> 00:17:07,990
假设你有一些特殊的应用程序或是采集程序
system is to you know you have some

378
00:17:07,990 --> 00:17:09,398
假设你有一些特殊的应用程序或是采集程序
particular application or mining you

379
00:17:09,398 --> 00:17:11,260
你可以编写一个专用的存储系统 专门适应这些特别的应用程序
build storage sort of dedicated and

380
00:17:11,260 --> 00:17:13,119
你可以编写一个专用的存储系统 专门适应这些特别的应用程序
tailored to that application and if

381
00:17:13,119 --> 00:17:14,829
如果你隔壁办公室的人也需要用到大型存储
somebody else in the next office needs

382
00:17:14,829 --> 00:17:17,079
他们就得自己去编写自己的存储系统 而无法复用你的程序
big storage well they can build their

383
00:17:17,079 --> 00:17:17,680
他们就得自己去编写自己的存储系统 而无法复用你的程序
own thing

384
00:17:17,680 --> 00:17:21,099
你要是有一个通用的 Global 且可利用的存储系统
right but if you have a universal server

385
00:17:21,099 --> 00:17:25,300
你要是有一个通用的 Global 且可利用的存储系统
kind of global reusable storage system

386
00:17:25,300 --> 00:17:28,030
这意味着 如果我存储大量从 web 中抓取的数据
and that means that if I store a huge

387
00:17:28,030 --> 00:17:29,710
这意味着 如果我存储大量从 web 中抓取的数据
amount of data set you know I'm crawling

388
00:17:29,710 --> 00:17:31,600
并且你也想查看我抓取的 web 页
the web and you want to look at my

389
00:17:31,600 --> 00:17:35,290
并且你也想看我抓取的 web 页
crawled web web pages because we're all

390
00:17:35,290 --> 00:17:36,580
因为我们都在相同的沙盒中折腾
using we're all playing in the same

391
00:17:36,580 --> 00:17:38,740
且使用了相同的存储系统
sandbox we're all using the same storage

392
00:17:38,740 --> 00:17:40,750
只要访问控制允许的话 你就可以读取我存的文件
system you can just read my files you

393
00:17:40,750 --> 00:17:43,480
只要访问控制允许的话 你就可以读取我存的文件
know maybe access controls permitting so

394
00:17:43,480 --> 00:17:45,190
所以就有了构建文件系统的想法
the idea was to build a sort of file

395
00:17:45,190 --> 00:17:47,110
任何在 Google 的人 都可以给任何文件命名或是读取它
system where anybody you know anybody

396
00:17:47,110 --> 00:17:50,080
任何在 Google 的人 都可以给任何文件命名或是读取它
inside Google could name and read any of

397
00:17:50,080 --> 00:17:57,010
目的就是为了共享
the files to allow sharing in order to

398
00:17:57,010 --> 00:17:58,540
为了能得到大容量 速度快的特性
get a in order to get bigness and

399
00:17:58,540 --> 00:18:00,300
他们需要把数据进行分割
fastness they need to split the data

400
00:18:00,300 --> 00:18:04,990
每个文件都将自动地被 GFS 分割到许多 server 上
through every file will be automatically

401
00:18:04,990 --> 00:18:07,900
每个文件都将自动地被 GFS 分割到许多 server 上
split by GFS over many servers so that

402
00:18:07,900 --> 00:18:08,950
只要有许多客户端大量读取文件 这样读写操作将会自动变快（注：并行化读写）
writes and reads would just

403
00:18:08,950 --> 00:18:10,780
只要有许多客户端大量读取文件 这样读写操作将会自动变快（注：并行化读写）
automatically be fast as long as you

404
00:18:10,780 --> 00:18:12,730
只要有许多客户端大量读取文件 这样读写操作将会自动变快（注：并行化读写）
were reading from lots and lots of

405
00:18:12,730 --> 00:18:14,770
只要有许多客户端大量读取文件 这样读写操作将会自动变快（注：并行化读写）
reading a file from lots of clients you

406
00:18:14,770 --> 00:18:17,860
你就能获得更高的呑吐量
get higher aggregate throughput and also

407
00:18:17,860 --> 00:18:20,230
而且能够让单个文件比单个磁盘还要大
be able to for a single file be able to

408
00:18:20,230 --> 00:18:21,670
而且能够让单个文件比单个磁盘还要大
have single files that were bigger than

409
00:18:21,670 --> 00:18:24,730
因为我们构建的东西在数百台服务器之上
any single disk because we're building

410
00:18:24,730 --> 00:18:26,170
因为我们构建的东西在数百台服务器之上
something out of hundreds of servers we

411
00:18:26,170 --> 00:18:36,430
所以我们希望这些服务器能够自动的从错误中恢复
want automatic failure recovery we

412
00:18:36,430 --> 00:18:37,480
如果数百台机器中 每时每刻都有机器发生故障
don't want to build a system where every

413
00:18:37,480 --> 00:18:38,860
如果数百台机器中 每时每刻都有机器发生故障
time one of our hundreds of servers a

414
00:18:38,860 --> 00:18:40,540
这时候你必须得有个人跑到机器去 对着 server 干一些事情
fail some human being has to go to the

415
00:18:40,540 --> 00:18:42,490
这时候你必须得有个人跑到机器去 对着 server 干一些事情
machine room and do something with the

416
00:18:42,490 --> 00:18:44,830
比如重启它 或者是迁移数据 或是别的什么也好
server or to get it up and running or

417
00:18:44,830 --> 00:18:46,870
比如重启它 或者是迁移数据 或是别的什么也好
transfers data or something well this

418
00:18:46,870 --> 00:18:50,130
你肯定不愿意用这样的服务 是吧 它都不能自我修复
isn't just fix itself um there were some

419
00:18:50,130 --> 00:18:54,370
还有一些并非是目标 比如 GFS 被设计成只在单数据中心上运行
sort of non goals like one is that GFS

420
00:18:54,370 --> 00:18:55,930
还有一些并非是目标 比如 GFS 被设计成只在单数据中心上运行
was designed to run in a single data

421
00:18:55,930 --> 00:18:57,340
我们并不会讨论如何把副本放到世界各地
center so we're not talking about

422
00:18:57,340 --> 00:18:59,950
我们并不会讨论如何把副本放到世界各地
placing replicas all over the world a

423
00:18:59,950 --> 00:19:02,410
单个 GFS 只安装在一个大机房里 只会运行在一个数据中心上
single GFS installation just lived in

424
00:19:02,410 --> 00:19:05,200
单个 GFS 只安装在一个大机房里 只会运行在一个数据中心上
one one data center one big machine room

425
00:19:05,200 --> 00:19:12,190
让这样的系统能够在副本彼此相距甚远的情况下工作
so getting this style system to work

426
00:19:12,190 --> 00:19:14,860
让这样的系统能够在副本彼此相距甚远的情况下工作
with the replicas are far distant from

427
00:19:14,860 --> 00:19:17,550
是一个有价值的目标 但是这还是挺难的
each other is a valuable goal but

428
00:19:17,550 --> 00:19:22,720
这个单数据中心并不是面向客户的服务
difficult so single data centers this is

429
00:19:22,720 --> 00:19:25,540
这个单数据中心并不是面向客户的服务
not a service to customers GFS was for

430
00:19:25,540 --> 00:19:27,920
GFS 是 Google 工程师开发的内部使用的工具
internal use by

431
00:19:27,920 --> 00:19:30,210
GFS 是 Google 工程师开发的内部使用的工具
applications written by Google engineers

432
00:19:30,210 --> 00:19:32,400
所以他们并不会直接出售这套程序
so it wasn't they weren't directly

433
00:19:32,400 --> 00:19:33,810
但他们可能会出售他们使用的服务
selling this they might be selling

434
00:19:33,810 --> 00:19:37,170
GFS 是内部使用的 他们不会直接出售的
services they used GFS internally but

435
00:19:37,170 --> 00:19:38,520
GFS 是内部使用的 他们不会直接出售的
they weren't selling it directly so it's

436
00:19:38,520 --> 00:19:45,660
GFS 是内部使用的 他们不会直接出售的
just for internal use and it was

437
00:19:45,660 --> 00:19:48,630
而且它是为大型顺序文件读写以多种方式定制的
tailored in a number of ways for big

438
00:19:48,630 --> 00:19:51,180
而且它是为大型顺序文件读写以多种方式定制的
sequential file reads and writes there's

439
00:19:51,180 --> 00:19:54,180
有一个完全不同的领域 比如为小块数据专门优化的存储系统
a whole nother domain like a system of

440
00:19:54,180 --> 00:19:56,490
有一个完全不同的领域 比如为小块数据专门优化的存储系统
storage systems that are optimized for

441
00:19:56,490 --> 00:19:58,590
有一个完全不同的领域 比如为小块数据专门优化的存储系统
small pieces of data like a bank that's

442
00:19:58,590 --> 00:20:00,090
比如银行余额需要有一个数据库
holding bank balances probably wants a

443
00:20:00,090 --> 00:20:02,100
它可以读、写和更新 100 字节存有人们银行余额的记录
database that can read and write and

444
00:20:02,100 --> 00:20:04,380
它可以读、写和更新 100 字节存有人们银行余额的记录
update you know 100 byte records that

445
00:20:04,380 --> 00:20:07,230
它可以读、写和更新 100 字节存有人们银行余额的记录
hold people's bank balances but GFS is

446
00:20:07,230 --> 00:20:10,230
GFS 可不是这样的系统 它就是为了处理“大文件”
not that system so it's really for big

447
00:20:10,230 --> 00:20:12,600
比如 GB TB 大小的文件
or big is you know terabytes gigabytes

448
00:20:12,600 --> 000:20:22,640
它只处理大文件的顺序访问 而不是随机访问
some big sequential not random access

449
00:20:22,640 --> 00:20:24,690
从某种程度上讲 它有点像批处理的风格
it's also that has a certain batch

450
00:20:24,690 --> 00:20:26,340
它没有花费过多的精力让延迟变的更低
flavor there's not a huge amount of

451
00:20:26,340 --> 00:20:27,840
它没有花费过多的精力让延迟变的更低
effort to make access be very low

452
00:20:27,840 --> 00:20:30,000
而是把重点放在具大的呑吐量上
latency the focus is really on

453
00:20:30,000 --> 00:20:32,880
而是把重点放在大呑吐量上
throughput of big you know multi

454
00:20:32,880 --> 00:20:36,780
比如数兆字节的操作
megabyte operations this paper was

455
00:20:36,780 --> 00:20:39,560
这篇论文 2003 年发表在 SOSP 上
published at SOSP in 2003 the top

456
00:20:39,560 --> 00:20:46,860
一个顶级系统学术会议
system's academic conference yeah usually

457
00:20:46,860 --> 00:20:49,080
通常在这样的会议上 论文标准是需要有很多创新研究
the standard for papers such conferences

458
00:20:49,080 --> 00:20:51,260
通常在这样的会议上 论文标准是需要有很多创新研究
they have you know a lot of very novel

459
00:20:51,260 --> 00:20:54,060
这些创新研究在课堂上一定不会有的
research this paper was not necessarily

460
00:20:54,060 --> 00:20:55,920
这篇论文中的点子 在那个时候并不特别新颖
in that class the specific ideas in this

461
00:20:55,920 --> 00:20:57,750
这篇论文中的点子 在那个时候并不特别新颖
paper none of them are particularly new

462
00:20:57,750 --> 00:21:00,990
像分布式、分片(sharding)、容错这些
at the time and things like distribution

463
00:21:00,990 --> 00:21:02,510
像分布式、分片(sharding)、容错这些
and sharding and fault tolerance were

464
00:21:02,510 --> 00:21:05,340
你能很好的理解并实现它们
you know well understood how to

465
00:21:05,340 --> 00:21:07,620
但是这篇论文描述的系统 是建立在成百上千台机器上的
deliver those but this paper described a

466
00:21:07,620 --> 00:21:09,480
但是这篇论文描述的系统 是建立在成百上千台机器上的
system that was really operating in in

467
00:21:09,480 --> 00:21:11,970
但是这篇论文描述的系统 是建立在成百上千台机器上的
use at a far far larger scale hundreds

468
00:21:11,970 --> 00:21:13,680
但是这篇论文描述的系统 是建立在成百上千台机器上的
of thousands of machines much bigger

469
00:21:13,680 --> 00:21:16,400
数量远远超过了以往学术界建立的系统所使用的机器
than any you know academics ever built

470
00:21:16,400 --> 00:21:18,960
事实上 它被用于工业界 并折射现实世界的经验
the fact that it was used in industry

471
00:21:18,960 --> 00:21:21,450
事实上 它被用于工业界 并折射现实世界的经验
and reflected real world experience of

472
00:21:21,450 --> 00:21:23,370
比如 对于已经部署的系统  什么不该做 什么该做 
like what actually didn't didn't work

473
00:21:23,370 --> 00:21:25,490
比如 对于已经部署的系统  什么不该做 什么该做
for deployed systems that had to work

474
00:21:25,490 --> 00:21:28,950
什么样的开销是最有效率的 这些都是非常有价值的
and had to be cost effective also like

475
00:21:28,950 --> 00:21:34,080
什么样的开销是最有效率的 这些都是非常有价值的
extremely valuable the paper sort of

476
00:21:34,080 --> 00:21:39,090
这篇论文提出了一个相当异端的观点
proposed a fairly heretical view that it

477
00:21:39,090 --> 00:21:40,800
认为存储系统具有弱一致性是可以接受的
was okay for the storage system to have

478
00:21:40,800 --> 00:21:41,270
认为存储系统具有弱一致性是可以接受的
pretty weak consistency

479
00:21:41,270 --> 00:21:45,440
在那时候 学术界的观念认为存储系统就应该具有良好的行为
we the academic mindset at

480
00:21:45,440 --> 00:21:46,550
在那时候 学术界的观念认为存储系统就应该具有良好的行为
that time was the you know the storage

481
00:21:46,550 --> 00:21:47,780
在那时候 学术界的观念认为存储系统就应该具有良好的行为
system really should have good behavior

482
00:21:47,780 --> 00:21:48,830
像这里的这个糟糕的复制系统 它返回了错误的数据
like what's the point of building

483
00:21:48,830 --> 00:21:50,780
像这里的这个糟糕的复制系统 它返回了错误的数据
systems that sort of return the wrong

484
00:21:50,780 --> 00:21:53,750
像这里的这个糟糕的复制系统 它返回了错误的数据
data like my terrible replication system

485
00:21:53,750 --> 00:21:55,400
它为什么要这样做 而不是构建一个能返回正确数据的系统
like why do that why not build systems

486
00:21:55,400 --> 00:21:57,020
它为什么要这样做 而不是构建一个能返回正确数据的系统
return the right data correct data

487
00:21:57,020 --> 00:21:59,240
它为什么要这样做 而不是构建一个能返回正确数据的系统
instead of incorrect data now with this

488
00:21:59,240 --> 00:22:02,570
在这篇论文里 它确实没有保证返回的正确的数据
paper actually does not guarantee return

489
00:22:02,570 --> 00:22:05,960
它的目的是获取更好的性能
correct data and you know the hope is

490
00:22:05,960 --> 00:22:07,130
它的目的是获取更好的性能
that they take advantage of that in

491
00:22:07,130 --> 00:22:09,440
它的目的是获取更好的性能
order to get better performance I'm a

492
00:22:09,440 --> 00:22:11,900
最后一件事 这篇论文里 使用的是单个 master
final thing that was sort of interesting

493
00:22:11,900 --> 00:22:13,580
最后一件事 这篇论文里 使用的是单个 master
about this paper is its use of a single

494
00:22:13,580 --> 00:22:16,370
在学术论文里 你可能会有自动容错的副本 用以恢复 master
master in a sort of academic paper you

495
00:22:16,370 --> 00:22:18,020
在学术论文里 你可能会有自动容错的副本 用以恢复 master
probably have some fault-tolerant

496
00:22:18,020 --> 00:22:20,900
在学术论文里 你可能会有自动容错的副本 用以恢复 master
replicated automatic failure recovering

497
00:22:20,900 --> 00:22:24,110
或许有许多的 master 进行分工
master perhaps many masters with the

498
00:22:24,110 --> 00:22:25,550
但是在这篇论文里
work split open um but this paper said

499
00:22:25,550 --> 00:22:26,960
它得以侥幸的只使用一个 master 并且能工作的很好
look you know you they can get away with

500
00:22:26,960 --> 00:22:39,260
它得以侥幸的只使用一个 master 并且能工作的很好
a single master and it worked fine well

501
00:22:39,260 --> 00:22:40,610
(提问) 很讽刺的是 谁关心 web 页面上投票的数量是否正确呢
cynically you know who's going to notice

502
00:22:40,610 --> 00:22:43,010
(提问) 很讽刺的是 谁关心 web 页面上投票的数量是否正确呢
on the web that some vote count or

503
00:22:43,010 --> 00:22:44,920
又或是别的什么错误呢
something is wrong or if you do a search

504
00:22:44,920 --> 00:22:47,510
如果你在搜索引擎上执行搜索
on a search engine now you're gonna know

505
00:22:47,510 --> 00:22:50,480
可能会发生 20000 条结果里有一条搜索结果丢失这样的事情
that oh you know like one of 20,000

506
00:22:50,480 --> 00:22:51,890
可能会发生 20000 条结果里有一条搜索结果丢失这样的事情
items is missing from the search results

507
00:22:51,890 --> 00:22:54,860
这些结果的排序可能也不正确
or they're in the wrong order probably

508
00:22:54,860 --> 00:22:58,130
所以在这类系统中 它对错误数据容错能力不需要像银行系统那样高
not so there was just much more

509
00:22:58,130 --> 00:22:59,510
所以在这类系统中 它对错误数据容错能力不需要像银行系统那样高
tolerance in these kind of systems than

510
00:22:59,510 --> 00:23:02,210
所以在这类系统中 它对错误数据容错能力不需要像银行系统那样高
there would like in a bank for incorrect

511
00:23:02,210 --> 00:23:04,070
这并不是说所有数据所有的 web 网站都可以是错误的
data it doesn't mean that all data and

512
00:23:04,070 --> 00:23:05,630
这并不是说所有数据所有的 web 网站都可以是错误的
websites can be wrong like if you're

513
00:23:05,630 --> 00:23:07,880
比如你收了别人的钱 给人家打广告
charging people for ad impressions you

514
00:23:07,880 --> 00:23:09,890
你最好还是得保证数字的正确性 这不一定就是这样哈
better get the numbers right but this is

515
00:23:09,890 --> 00:23:15,830
另外 GFS 中有一些提供数据的方法 可以在应用程序中进行补偿
not really about that in addition some

516
00:23:15,830 --> 00:23:18,370
另外 GFS 中有一些提供数据的方法 可以在应用程序中进行补偿
of the ways in which GFS could serve up

517
00:23:18,370 --> 00:23:21,770
另外 GFS 中有一些提供数据的方法 可以在应用程序中进行补偿
all data could be compensated for in the

518
00:23:21,770 --> 00:23:23,540
比如论文中介绍的 应用程序应当把数据及其校验和一起使用
applications like where the paper says

519
00:23:23,540 --> 00:23:25,490
比如论文中介绍的 应用程序应当把数据及其校验和一起使用
you know applications should accompany

520
00:23:25,490 --> 00:23:28,040
比如论文中介绍的 应用程序应当把数据及其校验和一起使用
their data with checksums and clearly

521
00:23:28,040 --> 00:23:30,260
并且要清晰的标记记录的边界
mark record boundaries that's so the

522
00:23:30,260 --> 00:23:32,380
所以应用程序就能从 GFS 中恢复数据
applications can recover from GFS

523
00:23:32,380 --> 00:23:40,970
提供服务可能并不需要强烈保证数据的正确性	
serving them maybe not quite the right data

525
00:23:40,970 --> 00:23:44,730
好，所以一般的结构是……就在这篇论文的图 1 里
all right so the general structure and

526
00:23:44,730 --> 00:23:48,840
好，所以一般的结构是……就在这篇论文的图 1 里
this is just figure one in the paper so

527
00:23:48,840 --> 00:23:53,850
我们有很多客户端 上百个吧 还有一个 master
we have a bunch of clients hundreds

528
00:23:53,850 --> 00:23:57,920
我们有很多客户端 上百个吧 还有一个 master
hundreds of clients we have one master

529
00:23:59,450 --> 00:24:02,040
尽管存在 master 的多个副本
although there might be replicas of the

530
00:24:02,040 --> 00:24:07,140
在这里 master 保存有从文件名到数据存储位置的映射
master the master keeps the mapping from

531
00:24:07,140 --> 00:24:09,510
在这里 master 保存有从文件名到数据存储位置的映射
file names to where to find the data

532
00:24:09,510 --> 00:24:10,980
实际上是有 2 张表
basically although there's really two

533
00:24:10,980 --> 00:24:14,100
然后 还有许多的 chunk server（块服务器）
tables so and then there's a bunch of

534
00:24:14,100 --> 00:24:18,390
然后 还有许多的 chunk server（块服务器）
chunk servers maybe hundreds of chunk

535
00:24:18,390 --> 00:24:21,090
可能会有上百个 每一个可能都会有一两块磁盘
servers each with perhaps one or two

536
00:24:21,090 --> 00:24:23,640
这里的 master 用来管理命名和追踪 chunk 的位置
disks the separation here's the master

537
00:24:23,640 --> 00:24:25,320
这里的 master 用来管理命名和追踪 chunk 的位置
is all about naming and knowing where

538
00:24:25,320 --> 00:24:27,480
chunk server 会存储实际的数据
the chunks are and the chunk servers

539
00:24:27,480 --> 00:24:29,400
chunk server 会存储实际的数据
store the actual data this is like a

540
00:24:29,400 --> 00:24:31,020
这看起来是设计中最好的地方了
nice aspect of the design that these two

541
00:24:31,020 --> 00:24:32,760
这两个地方完全彼此隔离起来
concerns are almost completely separated

542
00:24:32,760 --> 00:24:35,880
并能使用独立的特性进行独立设计
from each other and can be designed just

543
00:24:35,880 --> 00:24:41,700
并能使用独立的特性进行独立设计
separately with separate properties the

544
00:24:41,700 --> 00:24:43,170
master 知道所有的文件 并且能追踪这些 chunk
master knows about all the files for

545
00:24:43,170 --> 00:24:44,970
master 知道所有的文件 并且能追踪这些 chunk、chunk 标识符
every file the master keeps track of a

546
00:24:44,970 --> 00:24:48,260
master 知道所有的文件 并且能追踪这些 chunk、chunk 标识符
list of chunks chunk identifiers that

547
00:24:48,260 --> 00:24:50,880
chunk 中包含文件的连续片断 每个 chunk 大小是 64MB
contain the successive pieces that file

548
00:24:50,880 --> 00:24:53,400
chunk 中包含文件的连续片断 每个 chunk 大小是 64MB
each chunk is 64 megabytes so if I have

549
00:24:53,400 --> 00:24:57,090
如果我有 1GB 的文件
a you know gigabyte file the master is

550
00:24:57,090 --> 00:24:58,590
master 就会知道关于这个文件的第 1 个 chunk 保存在这里
gonna know that maybe the first chunk is

551
00:24:58,590 --> 00:25:00,059
第 2 个 chunk 保存在这里
stored here and the second chunk is

552
00:25:00,059 --> 00:25:01,559
第 3 个 chunk 保存在这里
stored here the third chunk is stored

553
00:25:01,559 --> 00:25:03,780
如果我想读取这个文件中的任意一部分
here and if I want to read whatever part

554
00:25:03,780 --> 00:25:05,490
我需要询问 master
of the file I need to ask the master oh

555
00:25:05,490 --> 00:25:07,260
which server hole is that chunk and I go

556
00:25:07,260 --> 00:25:09,000
talk to that server and read the chunk

557
00:25:09,000 --> 00:25:17,130
roughly speaking all right so more

558
00:25:17,130 --> 00:25:21,150
precisely we need to turns out if we're

559
00:25:21,150 --> 00:25:23,190
going to talk about how the system about

560
00:25:23,190 --> 00:25:24,690
the consistency of the system and how it

561
00:25:24,690 --> 00:25:27,360
deals with fault we need to know what

562
00:25:27,360 --> 00:25:29,100
the master is actually storing in a

563
00:25:29,100 --> 00:25:31,770
little bit more detail so the master

564
00:25:31,770 --> 00:25:34,190
data

565
00:25:36,190 --> 00:25:38,900
it's got two main tables that we care

566
00:25:38,900 --> 00:25:41,360
about it's got one table that map's file

567
00:25:41,360 --> 00:25:52,460
name to an array of chunk IDs or chunk

568
00:25:52,460 --> 00:26:00,830
handles this just tells you where to

569
00:26:00,830 --> 00:26:03,050
find the data or what the what the

570
00:26:03,050 --> 00:26:05,030
identifiers are the chunks are so it's

571
00:26:05,030 --> 00:26:06,620
not much yet you can do with a chunk

572
00:26:06,620 --> 00:26:08,840
identifier but the master also happens

573
00:26:08,840 --> 00:26:11,440
to have a a second table that map's

574
00:26:11,440 --> 00:26:17,570
chunk handles each chunk handle to a

575
00:26:17,570 --> 00:26:21,110
bunch of data about that chunk so one is

576
00:26:21,110 --> 00:26:23,330
the list of chunk servers that hold

577
00:26:23,330 --> 00:26:25,900
replicas of that data each chunk is

578
00:26:25,900 --> 00:26:28,040
stored on more than one chunk server so

579
00:26:28,040 --> 00:26:39,650
it's a list of chunk servers every chunk

580
00:26:39,650 --> 00:26:42,400
has a current version number so this

581
00:26:42,400 --> 00:26:46,610
master has a remembers the version

582
00:26:46,610 --> 00:26:50,150
number for each chunk all writes for a

583
00:26:50,150 --> 00:26:51,950
chunk have to be sequence of the chunks

584
00:26:51,950 --> 00:26:54,910
primary it's one of the replicas so

585
00:26:54,910 --> 00:26:58,880
master remembers the which chunk server's

586
00:26:58,880 --> 00:27:00,980
the primary and there's also that

587
00:27:00,980 --> 00:27:02,570
primary is only allowed to be primary

588
00:27:02,570 --> 00:27:05,450
for a certain lease time so the master

589
00:27:05,450 --> 00:27:13,370
remembers the expiration time of the

590
00:27:13,370 --> 00:27:17,240
lease this stuff so far it's all in RAM

591
00:27:17,240 --> 00:27:19,670
and the master so just be gone if the

592
00:27:19,670 --> 00:27:24,530
master crashed so in order that you'd be

593
00:27:24,530 --> 00:27:26,570
able to reboot the master and not forget

594
00:27:26,570 --> 00:27:29,150
everything about the file system the

595
00:27:29,150 --> 00:27:30,710
master actually stores all of this data

596
00:27:30,710 --> 00:27:35,180
on disk as well as in memory so reads

597
00:27:35,180 --> 00:27:38,270
just come from memory but writes to at

598
00:27:38,270 --> 00:27:40,490
least the parts of this data that had to

599
00:27:40,490 --> 00:27:42,140
be reflected on this writes have to go

600
00:27:42,140 --> 00:27:45,500
to the disk so and the way it actually

601
00:27:45,500 --> 00:27:47,510
managed that is that there's all

602
00:27:47,510 --> 00:27:51,290
the master has a log on disk and every

603
00:27:51,290 --> 00:27:53,750
time it changes the data it appends an

604
00:27:53,750 --> 00:27:59,380
entry to the log on disk and checkpoint

605
00:28:04,480 --> 00:28:07,220
so some of this stuff actually needs to

606
00:28:07,220 --> 00:28:10,600
be on disk and some doesn't it turns out

607
00:28:10,600 --> 00:28:12,980
I'm guessing a little bit here but

608
00:28:12,980 --> 00:28:16,190
certainly the array of chunk handles has

609
00:28:16,190 --> 00:28:18,050
to be on disk and so I'm gonna write env

610
00:28:18,050 --> 00:28:20,510
here for non-volatile meaning it it's

611
00:28:20,510 --> 00:28:22,850
got to be reflected on disk the list of

612
00:28:22,850 --> 00:28:25,610
chunk servers it turns out doesn't

613
00:28:25,610 --> 00:28:28,370
because the master if it reboots talks

614
00:28:28,370 --> 00:28:29,720
to all the chunk servers and ask them

615
00:28:29,720 --> 00:28:32,710
what chunks they have so this is I

616
00:28:32,710 --> 00:28:36,290
imagine not written to disk the version

617
00:28:36,290 --> 00:28:38,450
number any guesses written to disk not

618
00:28:38,450 --> 00:28:42,950
written to disk requires knowing how the

619
00:28:42,950 --> 00:28:51,830
system works I'm gonna vote written to

620
00:28:51,830 --> 00:28:55,790
disk non-volatile we can argue about

621
00:28:55,790 --> 00:28:57,500
that later when we talk about how system

622
00:28:57,500 --> 00:29:04,790
works identity the primary it turns out

623
00:29:04,790 --> 00:29:06,560
not almost certainly not written to disk

624
00:29:06,560 --> 00:29:10,640
so volatile and the reason is the master

625
00:29:10,640 --> 00:29:13,010
is um reboots and forgets therefore

626
00:29:13,010 --> 00:29:15,680
since it's volatile forgets who the

627
00:29:15,680 --> 00:29:17,330
primary is for a chunk it can simply

628
00:29:17,330 --> 00:29:19,910
wait for the 60-second lease expiration time

629
00:29:19,910 --> 00:29:21,920
and then it knows that absolutely no

630
00:29:21,920 --> 00:29:23,540
primary will be functioning for this

631
00:29:23,540 --> 00:29:24,920
chunk and then it can designate a

632
00:29:24,920 --> 00:29:27,020
different primary safely and similarly

633
00:29:27,020 --> 00:29:29,660
the lease expiration stuff is volatile

634
00:29:29,660 --> 00:29:32,840
so that means that whenever a file is

635
00:29:32,840 --> 00:29:35,030
extended with a new chunk goes to the

636
00:29:35,030 --> 00:29:40,100
next 64 megabyte boundary or the version

637
00:29:40,100 --> 00:29:42,710
number changes because the new primary

638
00:29:42,710 --> 00:29:45,740
is designated that means that the master

639
00:29:45,740 --> 00:29:48,440
has to first append a little record to

640
00:29:48,440 --> 00:29:50,900
his log basically saying oh I just added

641
00:29:50,900 --> 00:29:53,510
a such-and-such a chunk to this file or

642
00:29:53,510 --> 00:29:56,420
I just changed the version number so

643
00:29:56,420 --> 00:29:57,530
every time I change is one of those that

644
00:29:57,530 --> 00:29:59,360
needs to writes right it's disk so this

645
00:29:59,360 --> 00:30:00,830
is paper doesn't talk about this

646
00:30:00,830 --> 00:30:02,870
much but you know there's limits the

647
00:30:02,870 --> 00:30:05,090
rate at which the master can change

648
00:30:05,090 --> 00:30:07,039
things because you can only write your

649
00:30:07,039 --> 00:30:09,340
disk however many times per second and

650
00:30:09,340 --> 00:30:12,950
the reason for using a log rather than a

651
00:30:12,950 --> 00:30:16,279
database you know some sort of b-tree or

652
00:30:16,279 --> 00:30:20,179
hash table on disk is that you can

653
00:30:20,179 --> 00:30:23,980
append a log very efficiently because

654
00:30:24,010 --> 00:30:26,600
you only need you can take a bunch of

655
00:30:26,600 --> 00:30:28,309
recent log records they need to be added

656
00:30:28,309 --> 00:30:29,539
and sort of write them all on a single

657
00:30:29,539 --> 00:30:32,149
write after a single rotation to

658
00:30:32,149 --> 00:30:33,649
whatever the point in the disk is that

659
00:30:33,649 --> 00:30:36,080
contains the end of the log file whereas

660
00:30:36,080 --> 00:30:38,899
if it were a sort of b-tree reflecting

661
00:30:38,899 --> 00:30:42,080
the real structure of this data then you

662
00:30:42,080 --> 00:30:43,370
would have to seek to a random place in

663
00:30:43,370 --> 00:30:45,169
the disk and do a little right so the

664
00:30:45,169 --> 00:30:46,519
log makes a little bit faster to write

665
00:30:46,519 --> 00:30:51,620
there to reflect operations on to the

666
00:30:51,620 --> 00:30:56,570
disk however if the master crashes and

667
00:30:56,570 --> 00:30:58,789
has to reconstruct its state you

668
00:30:58,789 --> 00:31:00,409
wouldn't want to have to reread its log

669
00:31:00,409 --> 00:31:02,570
file back starting from the beginning of

670
00:31:02,570 --> 00:31:04,159
time from when the server was first

671
00:31:04,159 --> 00:31:06,559
installed you know a few years ago so in

672
00:31:06,559 --> 00:31:08,870
addition the master sometimes

673
00:31:08,870 --> 00:31:10,940
checkpoints its complete state to disk

674
00:31:10,940 --> 00:31:15,110
which takes some amount of time seconds

675
00:31:15,110 --> 00:31:17,779
maybe a minute or something and then

676
00:31:17,779 --> 00:31:20,210
when it restarts what it does is goes

677
00:31:20,210 --> 00:31:21,860
back to the most recent checkpoint and

678
00:31:21,860 --> 00:31:24,620
plays just the portion of a log that

679
00:31:24,620 --> 00:31:26,480
sort of starting at the point in time

680
00:31:26,480 --> 00:31:30,019
when that check one is created any

681
00:31:30,019 --> 00:31:39,340
questions about the master data okay

682
00:31:40,360 --> 00:31:44,029
so with that in mind I'm going to lay

683
00:31:44,029 --> 00:31:46,340
out the steps in a read and the steps in

684
00:31:46,340 --> 00:31:46,879
the right

685
00:31:46,879 --> 00:31:49,129
where all this is heading is that I then

686
00:31:49,129 --> 00:31:50,960
want to discuss you know for each

687
00:31:50,960 --> 00:31:53,840
failure I can think of why does the

688
00:31:53,840 --> 00:31:56,389
system or does the system act directly

689
00:31:56,389 --> 00:31:58,639
after that failure um but in order to do

690
00:31:58,639 --> 00:32:00,740
that we need to understand the data and

691
00:32:00,740 --> 00:32:03,470
operations in the data okay so if

692
00:32:03,470 --> 00:32:11,210
there's a read the first step is that

693
00:32:11,210 --> 00:32:12,980
the client and what a read means that

694
00:32:12,980 --> 00:32:14,749
the application has a file name in mind

695
00:32:14,749 --> 00:32:17,450
and an offset in the file that it wants

696
00:32:17,450 --> 00:32:19,279
to read some data front so it sends the

697
00:32:19,279 --> 00:32:21,799
file name and the offset to the master

698
00:32:21,799 --> 00:32:23,869
and the master looks up the file name in

699
00:32:23,869 --> 00:32:25,759
its file table and then you know each

700
00:32:25,759 --> 00:32:28,309
chunk is 64 megabytes who can use the

701
00:32:28,309 --> 00:32:30,889
offset divided by 64 megabytes to find

702
00:32:30,889 --> 00:32:33,649
which chunk and then it looks up that

703
00:32:33,649 --> 00:32:39,409
chunk in its chunk table finds the list

704
00:32:39,409 --> 00:32:41,869
of chunk servers that have replicas of

705
00:32:41,869 --> 00:32:44,509
that data and returns that list to the

706
00:32:44,509 --> 00:32:52,249
client so the first step is so you know

707
00:32:52,249 --> 00:32:56,809
the file name and the offset the master

708
00:32:56,809 --> 00:33:05,720
and the master sends the chunk handle

709
00:33:05,720 --> 00:33:11,450
let's say H and the list of servers so

710
00:33:11,450 --> 00:33:13,070
now we have some choice we can ask any

711
00:33:13,070 --> 00:33:15,590
one of these servers pick one that's and

712
00:33:15,590 --> 00:33:17,990
the paper says that clients try to guess

713
00:33:17,990 --> 00:33:19,429
which server is closest to them in the

714
00:33:19,429 --> 00:33:23,360
network maybe in the same rack and send

715
00:33:23,360 --> 00:33:27,279
the read request to that to that replica

716
00:33:28,480 --> 00:33:32,649
the client actually caches

717
00:33:35,550 --> 00:33:37,930
caches this result so that if it reads

718
00:33:37,930 --> 00:33:39,820
that chunk again and indeed the client

719
00:33:39,820 --> 00:33:41,560
might read a given chunk in you know one

720
00:33:41,560 --> 00:33:45,550
megabyte pieces or 64 kilobyte pieces or

721
00:33:45,550 --> 00:33:47,620
something so I may end up reading the

722
00:33:47,620 --> 00:33:49,410
same chunk different points successive

723
00:33:49,410 --> 00:33:51,730
regions of a chunk many times and so

724
00:33:51,730 --> 00:33:56,050
caches which server to talk to you for

725
00:33:56,050 --> 00:33:57,310
giving chunks so it doesn't have to keep

726
00:33:57,310 --> 00:33:59,020
beating on the master asking the master

727
00:33:59,020 --> 00:34:02,550
for the same information over and over

728
00:34:03,150 --> 00:34:07,330
now the client talks to one of the chunk

729
00:34:07,330 --> 00:34:12,880
servers tells us a chunk handling offset

730
00:34:12,880 --> 00:34:16,540
and the chunk servers store these chunks

731
00:34:16,540 --> 00:34:19,060
each chunk in a separate Linux file on

732
00:34:19,060 --> 00:34:21,340
their hard drive in a ordinary Linux

733
00:34:21,340 --> 00:34:24,699
file system and presumably the chunk

734
00:34:24,699 --> 00:34:26,800
files are just named by the handle so

735
00:34:26,800 --> 00:34:28,659
all the chunk server has to do is go

736
00:34:28,659 --> 00:34:31,210
find the file with the right name you

737
00:34:31,210 --> 00:34:33,449
know I'll give it that

738
00:34:33,449 --> 00:34:35,560
entire chunk and then just read the

739
00:34:35,560 --> 00:34:38,130
desired range of bytes out of that file

740
00:34:38,130 --> 00:34:46,570
and return the data to the client I hate

741
00:34:46,570 --> 00:34:51,909
question about how reads operate can I

742
00:34:51,909 --> 00:34:54,370
repeat number one the step one is the

743
00:34:54,370 --> 00:34:57,880
application wants to read it a

744
00:34:57,880 --> 00:35:00,040
particular file at a particular offset

745
00:35:00,040 --> 00:35:02,890
within the file a particular range of

746
00:35:02,890 --> 00:35:04,420
bytes in the files and one thousand two

747
00:35:04,420 --> 00:35:05,830
two thousand and so it just sends a name

748
00:35:05,830 --> 00:35:09,010
of the file and the beginning of the

749
00:35:09,010 --> 00:35:12,160
byte range to the master and then the

750
00:35:12,160 --> 00:35:14,050
master looks a file name and it's file

751
00:35:14,050 --> 00:35:18,610
table to find the chunk that contains

752
00:35:18,610 --> 00:35:23,820
that byte range for that file so good

753
00:35:30,980 --> 00:35:34,119
[Music]

754
00:35:34,150 --> 00:35:36,500
so I don't know the exact details my

755
00:35:36,500 --> 00:35:38,200
impression is that the if the

756
00:35:38,200 --> 00:35:40,309
application wants to read more than 64

757
00:35:40,309 --> 00:35:42,319
megabytes or even just two bytes but

758
00:35:42,319 --> 00:35:44,779
spanning a chunk boundary that the

759
00:35:44,779 --> 00:35:47,869
library so the applications linked with

760
00:35:47,869 --> 00:35:52,099
a library that sends our pcs to the

761
00:35:52,099 --> 00:35:54,230
various servers and that library would

762
00:35:54,230 --> 00:35:56,690
notice that the reads spanned a chunk

763
00:35:56,690 --> 00:35:58,490
boundary and break it into two separate

764
00:35:58,490 --> 00:36:01,039
reads and maybe talk to the master I

765
00:36:01,039 --> 00:36:02,480
mean it may be that you could talk to

766
00:36:02,480 --> 00:36:04,069
the master once and get two results or

767
00:36:04,069 --> 00:36:06,710
something but logically at least it two

768
00:36:06,710 --> 00:36:08,269
requests to the master and then requests

769
00:36:08,269 --> 00:36:19,609
to two different chunk servers yes well

770
00:36:19,609 --> 00:36:21,650
at least initially the client doesn't

771
00:36:21,650 --> 00:36:26,829
know for a given file

772
00:36:26,829 --> 00:36:35,990
what chunks need what chunks well it can

773
00:36:35,990 --> 00:36:37,720
calculate it needs the seventeenth chunk

774
00:36:37,720 --> 00:36:40,130
but but then it needs to know what chunk

775
00:36:40,130 --> 00:36:42,109
server holds the seventeenth chunk of

776
00:36:42,109 --> 00:36:44,839
that file and for that it certainly

777
00:36:44,839 --> 00:36:47,599
needs for that it needs to talk to the

778
00:36:47,599 --> 00:36:58,490
master okay so all right did I'm not

779
00:36:58,490 --> 00:36:59,839
going to make a strong claim about which

780
00:36:59,839 --> 00:37:01,130
of them decides that it was the

781
00:37:01,130 --> 00:37:03,170
seventeenth chunk in the file but it's

782
00:37:03,170 --> 00:37:06,380
the master that finds the identifier of

783
00:37:06,380 --> 00:37:07,849
the handle of the seventeenth chunk in

784
00:37:07,849 --> 00:37:09,950
the file looks that up in its table and

785
00:37:09,950 --> 00:37:12,589
figures out which chunk servers hold

786
00:37:12,589 --> 00:37:17,349
that chunk yes

787
00:37:25,609 --> 00:37:35,480
how does that or you mean if the if the

788
00:37:35,480 --> 00:37:38,010
client asks for a range of bytes that

789
00:37:38,010 --> 00:37:46,400
spans a chunk boundary yeah so the the

790
00:37:46,400 --> 00:37:49,049
well you know the client will ask that

791
00:37:49,049 --> 00:37:50,490
well the clients linked with this

792
00:37:50,490 --> 00:37:52,950
library is a GFS library that noticed

793
00:37:52,950 --> 00:37:56,190
how to take read requests apart and put

794
00:37:56,190 --> 00:38:00,270
them back together and so that library

795
00:38:00,270 --> 00:38:01,290
would talk to the master and the master

796
00:38:01,290 --> 00:38:02,910
would tell it well well you know chunk

797
00:38:02,910 --> 00:38:05,130
seven is on this server and chunk eight

798
00:38:05,130 --> 00:38:07,589
is on that server and then why the

799
00:38:07,589 --> 00:38:09,270
library would just be able to say oh you

800
00:38:09,270 --> 00:38:10,859
know I need the last couple bites of

801
00:38:10,859 --> 00:38:12,240
chunk seven and the first couple bites

802
00:38:12,240 --> 00:38:15,420
of chunk eight and then would fetch

803
00:38:15,420 --> 00:38:17,819
those put them together in a buffer and

804
00:38:17,819 --> 00:38:21,980
return them to the calling application

805
00:38:26,030 --> 00:38:28,530
well the master tells it about chunks

806
00:38:28,530 --> 00:38:30,900
and the library kind of figures out

807
00:38:30,900 --> 00:38:32,700
where it should look in a given chunk to

808
00:38:32,700 --> 00:38:34,950
find the date of the application wanted

809
00:38:34,950 --> 00:38:36,240
the application only thinks in terms of

810
00:38:36,240 --> 00:38:38,609
file names and sort of just offsets in

811
00:38:38,609 --> 00:38:41,280
the entire file in the library and the

812
00:38:41,280 --> 00:38:45,200
master conspire to turn that into chunks

813
00:38:45,500 --> 00:38:48,500
yeah

814
00:38:50,349 --> 00:38:55,400
sorry let me get closer here you say

815
00:38:55,400 --> 00:39:03,289
again so the question is does it matter

816
00:39:03,289 --> 00:39:06,109
which chunk server you read from so you

817
00:39:06,109 --> 00:39:08,929
know yes and no notionally they're all

818
00:39:08,929 --> 00:39:13,039
supposed to be replicas in fact as you

819
00:39:13,039 --> 00:39:14,869
may have noticed or as we'll talk about

820
00:39:14,869 --> 00:39:17,209
they're not you know they're not

821
00:39:17,209 --> 00:39:20,689
necessarily identical and applications

822
00:39:20,689 --> 00:39:21,979
are supposed to be able to tolerate this

823
00:39:21,979 --> 00:39:23,779
but the fact is that you make a slightly

824
00:39:23,779 --> 00:39:24,829
different data depending on which

825
00:39:24,829 --> 00:39:28,999
replicas you read yeah so the paper says

826
00:39:28,999 --> 00:39:32,420
that clients try to read from the chunk

827
00:39:32,420 --> 00:39:34,699
server that's in the same rack or on the

828
00:39:34,699 --> 00:39:44,749
same switch or something all right so

829
00:39:44,749 --> 00:39:47,229
that's reads

830
00:39:48,859 --> 00:39:51,420
the writes are more complex and

831
00:39:51,420 --> 00:40:02,880
interesting now the application

832
00:40:02,880 --> 00:40:04,410
interface for writes is pretty similar

833
00:40:04,410 --> 00:40:06,030
there's just some call some library you

834
00:40:06,030 --> 00:40:08,910
call to make you make to the gfs client

835
00:40:08,910 --> 00:40:10,230
library saying look here's a file name

836
00:40:10,230 --> 00:40:12,540
and a range of bytes I'd like to write

837
00:40:12,540 --> 00:40:14,339
and the buffer of data that I'd like you

838
00:40:14,339 --> 00:40:17,609
to write to that that range actually let

839
00:40:17,609 --> 00:40:19,530
me let me backpedal I only want to talk

840
00:40:19,530 --> 00:40:23,099
about record appends and so I'm going to

841
00:40:23,099 --> 00:40:26,339
praise this the client interface as the

842
00:40:26,339 --> 00:40:28,200
client makes a library call that says

843
00:40:28,200 --> 00:40:29,940
here's a file name and I'd like to

844
00:40:29,940 --> 00:40:32,069
append this buffer of bytes to the file

845
00:40:32,069 --> 00:40:35,099
I said this is the record appends that

846
00:40:35,099 --> 00:40:42,900
the paper talks about so again the

847
00:40:42,900 --> 00:40:47,579
client asks the master look I want to

848
00:40:47,579 --> 00:40:49,680
append sends a master requesting what I

849
00:40:49,680 --> 00:40:51,240
would like to append this named file

850
00:40:51,240 --> 00:40:55,140
please tell me where to look for the

851
00:40:55,140 --> 00:40:56,790
last chunk in the file because the

852
00:40:56,790 --> 00:40:58,619
client may not know how long the file is

853
00:40:58,619 --> 00:41:00,329
if lots of clients are appending the

854
00:41:00,329 --> 00:41:02,819
因为我们有一些大文件
same file because we have some big file

855
00:41:02,819 --> 00:41:04,950
比如许多不同客户端的日志
this logging stuff from a lot of

856
00:41:04,950 --> 00:41:06,900
比如许多不同客户端的日志
different clients may be you know 

857
00:41:06,900 --> 00:41:08,369
client不知道文件有多长
no client will necessarily know how long

858
00:41:08,369 --> 00:41:10,380
因此也不知道偏移量，或是应该附加到哪个块中
the file is and therefore which offset

859
00:41:10,380 --> 00:41:12,270
因此也不知道偏移量，或是应该附加到哪个块中
or which chunk it should be appending to

860
00:41:12,270 --> 00:41:14,280
你可以问master，请告诉我
so you can ask the master please tell me

861
00:41:14,280 --> 00:41:16,680
包含当前该文件最后一块的服务器的信息
about the the server's that hold the

862
00:41:16,680 --> 00:41:18,710
包含当前该文件最后一块的服务器的信息
very last chunk

863
00:41:18,710 --> 00:41:22,550
包含当前该文件最后一块的服务器的信息
current chunk in this file 

864
00:41:22,550 --> 00:41:26,040
so unfortunately now the writing 

865
00:41:26,040 --> 00:41:27,569
如果你在读取，你可以从任何最新副本中读取内容
if you're reading you can read from any up-to-date replica 

866
00:41:27,569 --> 00:41:30,060
对于写的情况，需要有一个primary
for writing though there needs

867
00:41:30,060 --> 00:41:32,760
此时文件上的primary
to be a primary so at this point on the

868
00:41:32,760 --> 00:41:35,579
可能已经由master指定
file may or may not have a primary

869
00:41:35,579 --> 00:41:37,710
也可能没有
already designated by the master

870
00:41:37,710 --> 00:41:39,180
所以我们需要考虑这个情况
so we need to consider the case of if there's

871
00:41:39,180 --> 00:41:40,980
如果还没有primary
no primary already and all the master

872
00:41:40,980 --> 00:41:49,560
而且master所知道的，好吧，没有primary
knows well there's no primary so 

873
00:41:49,560 --> 00:41:53,119
所以一种情况是没有primary
so one case is no primary

874
00:41:57,599 --> 00:42:00,099
在这种情况下，master服务器需要
in that case the master needs to find

875
00:42:00,099 --> 00:42:03,430
找出具有最新块副本的chunk服务器集合
out the set of chunk servers that have

876
00:42:03,430 --> 00:42:06,339
找出具有最新块副本的chunk服务器集合
the most up-to-date copy of the chunk

877
00:42:06,339 --> 00:42:08,470
因为，如果你是否已经运行了该系统很长时间
because know if you've been running the

878
00:42:08,470 --> 00:42:10,660
因为，如果你是否已经运行了该系统很长时间
system for a long time due to failures

879
00:42:10,660 --> 00:42:11,800
由于故障或其他原因
or whatever there may be chunk servers

880
00:42:11,800 --> 00:42:14,290
可能有的chunk服务器有旧的
out there that have old copies of the

881
00:42:14,290 --> 00:42:15,579
昨天或上周的副本
chunk from you know yesterday or last

882
00:42:15,579 --> 00:42:17,950
week that I've been kept up to kept up

883
00:42:17,950 --> 00:42:19,690
因为可能该服务器已挂了两天
to date because maybe that server was

884
00:42:19,690 --> 00:42:21,819
dead for a couple days and wasn't

885
00:42:21,819 --> 00:42:23,800
并且没有收到更新
receiving updates so there's you need to

886
00:42:23,800 --> 00:42:24,730
因此，你需要能够分辨出
be able to tell the difference between

887
00:42:24,730 --> 00:42:27,190
最新的数据块副本与非最新的副本之间的区别
up-to-date copies of the chunk and non up-to-date 

888
00:42:27,190 --> 00:42:33,569
所以第一步是要找到
so the first step is to find

889
00:42:33,569 --> 00:42:37,510
找到最新的
you know find up-to-date this is all

890
00:42:37,510 --> 00:42:41,319
这一切都发生在master
happening in the master because the

891
00:42:41,319 --> 00:42:42,790
因为client请求master
client has asked the master told the

892
00:42:42,790 --> 00:42:44,260
告诉master我要追加此文件
master look I want up end of this file

893
00:42:44,260 --> 00:42:46,180
请告诉我要与哪些块服务器对话
please tell me what chunk service to talk to 

894
00:42:46,180 --> 00:42:48,550
所以master服务器一部分
so a part of the master trying

895
00:42:48,550 --> 00:42:49,780
试图弄清楚client端
to figure out what chunk servers the

896
00:42:49,780 --> 00:42:50,680
应该与哪些服务器通信
client should talk to you

897
00:42:50,680 --> 00:42:52,950
所以当我们最终找到最新的副本时
so when we finally find up-to-date

898
00:42:52,950 --> 00:42:59,770
更新的意思是
replicas and what update means is a

899
00:42:59,770 --> 00:43:02,260
一个副本，它的块版本等于
replica whose version of the chunk is

900
00:43:02,260 --> 00:43:04,720
master服务器知道的最新版本号
equal to the version number that the

901
00:43:04,720 --> 00:43:06,730
master服务器知道的最新版本号
master knows is the most up-to-date version number 

902
00:43:06,730 --> 00:43:08,140
master服务器
it's the master that

903
00:43:08,140 --> 00:43:10,630
发出这些版本号
hands out these version numbers

904
00:43:10,630 --> 00:43:14,740
master记得，哦
the master remembers that oh for this

905
00:43:14,740 --> 00:43:18,460
对于这个特定的块
particular chunk you know the chunk

906
00:43:18,460 --> 00:43:19,569
仅当主干服务器的版本号为17时
server is only up to date if it has

907
00:43:19,569 --> 00:43:21,220
它才是最新的
version number 17 and this is why it has

908
00:43:21,220 --> 00:43:23,550
这就是为什么它必须是非易失性的存储在磁盘上的原因
to be non-volatile stored on disk

909
00:43:23,550 --> 00:43:26,560
因为如果它在一次崩溃中丢失
because if if it was lost in a crash and

910
00:43:26,560 --> 00:43:31,000
并且有块服务器保存过时的块副本
there were chunk servers holding stale

911
00:43:31,000 --> 00:43:33,670
master服务器
copies of chunks the master wouldn't be

912
00:43:33,670 --> 00:43:35,140
将无法区分
able to distinguish between chunk

913
00:43:35,140 --> 00:43:36,819
持有上周旧数据块副本的服务器
servers holding stale copies of a chunk

914
00:43:36,819 --> 00:43:39,310
和一个崩溃时
from last week and a chunk server that

915
00:43:39,310 --> 00:43:42,250
保存了最新块副本的服务器
holds the copy of the chunk that was

916
00:43:42,250 --> 00:43:44,440
这就是为什么
up-to-date as of the crash that's why

917
00:43:44,440 --> 00:43:46,660
master服务器把版本号记在磁盘上的的原因
the master remembers the version number on disk 

918
00:43:46,660 --> 00:43:49,470
yeah

919
00:43:54,450 --> 00:43:56,859
如果你正在与所有块服务器通信
if you knew you were talking to all the

920
00:43:56,859 --> 00:43:59,970
观察是
chunk servers okay so the observation is

921
00:43:59,970 --> 00:44:02,260
如果master重启
the master has to talk to the chunk

922
00:44:02,260 --> 00:44:04,660
master服务器无论如何都要与块服务器进行通信
servers anyway if it reboots in order to

923
00:44:04,660 --> 00:44:06,280
为了找到哪个块服务器持有哪个块
find which chunk server holds which chunk 

924
00:44:06,280 --> 00:44:08,890
因为master不记得那些信息了
because the master doesn't

925
00:44:08,890 --> 00:44:12,150
你可能会想
remember that so you might think that

926
00:44:12,150 --> 00:44:14,380
你可以取最大的值
you could just take the maximum you

927
00:44:14,380 --> 00:44:15,579
你可以和块服务器对话
could just talk to the chunk servers

928
00:44:15,579 --> 00:44:17,079
找出他们拥有哪些块和版本
find out what trunks and versions they

929
00:44:17,079 --> 00:44:19,000
并在响应的块服务器中获取最大值
hold and take the maximum for a given

930
00:44:19,000 --> 00:44:20,619
并在响应的块服务器中获取最大值
chunk overall the responding chunk

931
00:44:20,619 --> 00:44:22,750
如果所有持有块的块服务器都响应
servers and that would work if all the

932
00:44:22,750 --> 00:44:24,579
那会是有效的
chunk servers holding a chunk responded

933
00:44:24,579 --> 00:44:26,920
但风险是，主机重启时
but the risk is that at the time the

934
00:44:26,920 --> 00:44:28,480
也许一些块服务器
master reboots maybe some of the chunk

935
00:44:28,480 --> 00:44:30,400
离线或断开连接
servers are offline or disconnected or

936
00:44:30,400 --> 00:44:32,770
或无论什么原因，他们自己重新启动并且不响应
whatever themselves rebooting and don't respond

937
00:44:32,770 --> 00:44:35,349
因此，主服务器拿到的
and so all the master gets back

938
00:44:35,349 --> 00:44:38,200
都是来自具有上周旧的块副本的服务器的响应
is responses from chunk servers that

939
00:44:38,200 --> 00:44:40,119
都是来自具有上周旧的块副本的服务器的响应
have last week's copies of the block and

940
00:44:40,119 --> 00:44:42,460
而具有当前副本的块服务器
the chunk servers that have the current

941
00:44:42,460 --> 00:44:44,320
尚未完成开机
copy haven't finished rebooting or

942
00:44:44,320 --> 00:44:54,940
或是离线状态或是别的情况
offline or something so ok oh yes if 

943
00:44:54,940 --> 00:44:56,619
如果服务器保存的最新副本
if the server's holding the most recent

944
00:44:56,619 --> 00:44:59,859
永久失效，如果你已经
copy are permanently dead if you've lost

945
00:44:59,859 --> 00:45:02,980
丢失了所有副本，所有最新版本，那么是的
all copies all of the most recent

946
00:45:02,980 --> 00:45:06,540
丢失了所有副本，所有最新版本，那么是的
version of a chunk then yes

947
00:45:09,030 --> 00:45:11,130
No

948
00:45:11,130 --> 00:45:15,339
好吧，问题说的是，master知道
okay so the question is the master knows

949
00:45:15,339 --> 00:45:17,560
对于这个块寻找版本17
that for this chunk is looking for

950
00:45:17,560 --> 00:45:18,550
对于这个块寻找版本17
version 17

951
00:45:18,550 --> 00:45:21,579
假设它找不到块服务器
supposing it finds no chunk server you

952
00:45:21,579 --> 00:45:22,690
master定期与块服务器进行对话
know and it talks to the chunk servers

953
00:45:22,690 --> 00:45:24,430
以询问他们
periodically to sort of ask them what

954
00:45:24,430 --> 00:45:25,780
拥有哪些块，拥有什么版本
chunks do you have what versions you have 

955
00:45:25,780 --> 00:45:27,520
假设它没有块找到
supposing it finds no server with

956
00:45:27,520 --> 00:45:30,369
版本为17的块的服务器
chunk 17 with version 17 for this this

957
00:45:30,369 --> 00:45:32,800
master要么先不回复并等待
chunk then the master will either say

958
00:45:32,800 --> 00:45:35,710
master要么先不回复并等待
well either not respond yet and wait or

959
00:45:35,710 --> 00:45:39,790
要么告诉client，我无法回答，请稍后再试
it will tell the client look I can't

960
00:45:39,790 --> 00:45:42,880
要么告诉client，我无法回答，请稍后再试
answer that try again later and this

961
00:45:42,880 --> 00:45:44,530
就像建筑物中的电源故障
would come up like there was a power

962
00:45:44,530 --> 00:45:45,849
就像建筑物中的电源故障
failure in the building and all the

963
00:45:45,849 --> 00:45:47,079
所有服务器都崩溃了，我们正在缓慢地重新启动
server's crashed and we're slowly rebooting 

964
00:45:47,079 --> 00:45:49,510
master可能先启动完成
the master might come up first

965
00:45:49,510 --> 00:45:51,430
一些块服务器可能已启动
and you know some fraction of the chunk

966
00:45:51,430 --> 00:45:53,079
其他的会从现在起五分钟后重启
servers might be up and other ones would

967
00:45:53,079 --> 00:45:57,609
其他的会从现在起五分钟后重启
reboot five minutes from now but 

968
00:45:57,609 --> 00:45:59,890
所以我们必须准备等待
so we have to be prepared to wait and 

969
00:45:59,890 --> 00:46:02,290
它将永远等待，因为
it will wait forever because you don't want to

970
00:46:02,290 --> 00:46:05,440
你不想使用该块的陈旧版本
use a stale version of that of a chunk

971
00:46:05,440 --> 00:46:09,190
master服务器需要
okay so the master needs to assemble the

972
00:46:09,190 --> 00:46:10,540
整合具有最新版本的块服务器列表
list of chunk servers that have the most

973
00:46:10,540 --> 00:46:12,910
master服务器知道
recent version the master knows the most

974
00:46:12,910 --> 00:46:14,619
磁盘上存储的最新版本
recent versions stored on disk each

975
00:46:14,619 --> 00:46:16,540
正如你所指出的
chunk server along with each chunk as

976
00:46:16,540 --> 00:46:18,280
每个块服务器以及每个块
you pointed out also remembers the

977
00:46:18,280 --> 00:46:19,810
也记得它存储的块的版本号
version number of the chunk that it's stores 

978
00:46:19,810 --> 00:46:22,540
这样，当块服务器
so that when chunk servers

979
00:46:22,540 --> 00:46:23,859
向master服务器报告说，看，我有这个块
reported into the master saying look I

980
00:46:23,859 --> 00:46:25,690
master服务器可以
have this chunk the master can ignore

981
00:46:25,690 --> 00:46:27,760
忽略那些版本与master知道的最新版本不匹配的
the ones whose version does not match

982
00:46:27,760 --> 00:46:30,339
忽略那些版本与master知道的最新版本不匹配的
the version the master knows is the most recent 

983
00:46:30,339 --> 00:46:34,869
记得我们client想要追加
okay so remember we were the

984
00:46:34,869 --> 00:46:36,670
master服务器没有primary
client want to append the master doesn't

985
00:46:36,670 --> 00:46:39,579
master意识到
have a primary it figures out maybe you

986
00:46:39,579 --> 00:46:42,310
你可能需要等待具有该块最新版本的服务器集合
have to wait for the set of chunk

987
00:46:42,310 --> 00:46:43,960
你可能需要等待具有该块最新版本的服务器集合
servers that have the most recent

988
00:46:43,960 --> 00:46:49,020
它选择一个作为primary服务器
version of that chunk it picks a primary

989
00:46:50,040 --> 00:46:52,930
所以我要在最新版本的副本中
so I'm gonna pick one of them to be the

990
00:46:52,930 --> 00:46:56,859
选择其中一个作为primary服务器
primary and the others to be secondary servers

992
00:46:56,859 --> 00:46:58,210
将其他作为secondary服务器
among the replicas set at the most

993
00:46:58,210 --> 00:47:02,140
然后master
recent version the master then

994
00:47:02,140 --> 00:47:11,170
将版本号递增，并将其写入磁盘
increments the version number and writes that to

996
00:47:11,170 --> 00:47:13,600
所以崩溃时，它不会忘记（版本号）
disk so it doesn't forget it the crashes

997
00:47:13,600 --> 00:47:15,970
然后它发送给primary和secondary
and then it sends the primary and

998
00:47:15,970 --> 00:47:18,700
每个都有一个消息说
secondaries and that's each of them a

999
00:47:18,700 --> 00:47:20,710
对于这个块
message saying look for this chunk

1000
00:47:20,710 --> 00:47:22,840
这是primary这是secondary
here's the primary here's the

1001
00:47:22,840 --> 00:47:26,650
收件人可能是其中之一
secondaries you know recipient maybe one

1002
00:47:26,650 --> 00:47:28,450
消息里说，对于这个块，这是新版本号
of them and here's the new version

1003
00:47:28,450 --> 00:47:32,170
然后它会告诉primary，secondary此信息以及版本号
number so then it tells primary

1004
00:47:32,170 --> 00:47:37,060
此信息以及版本号
secondaries this information plus the

1005
00:47:37,060 --> 00:47:39,970
primary节点和secondary节点
version number the primaries and secondaries

1007
00:47:39,970 --> 00:47:41,920
都将版本号写入磁盘
all write the version number to disk 

1008
00:47:41,920 --> 00:47:43,780
他们不会忘记，因为
so they don't forget because you know 

1009
00:47:43,780 --> 00:47:45,040
如果有电源故障或其他任何原因
if there's a power failure or whatever 

1010
00:47:45,040 --> 00:47:47,140
他们必须用他们持有的版本号
they have to report in to the master with the

1011
00:47:47,140 --> 00:47:51,210
向master服务器报告
actual version number they hold yes

1012
00:48:04,230 --> 00:48:06,190
这是一个很好的问题
that's a great question

1013
00:48:06,190 --> 00:48:08,500
我不知道，文章中有个暗示
so I don't know there's hints in the

1014
00:48:08,500 --> 00:48:11,170
我对此略有错误
paper that I'm slightly wrong about this

1015
00:48:11,170 --> 00:48:14,740
我认为你的问题
so the paper says I think your question

1016
00:48:14,740 --> 00:48:16,030
向我解释了有关论文的一些事情
was explaining something to me about the

1017
00:48:16,030 --> 00:48:18,310
该论文说
paper the paper says if the master

1018
00:48:18,310 --> 00:48:22,480
如果master服务器重启并与块服务器进行对话
reboots and talks to chunk servers and

1019
00:48:22,480 --> 00:48:24,220
并且其中一个块服务器重启时报告的版本号
one of the chunk servers reboot reports

1020
00:48:24,220 --> 00:48:26,530
高于master服务器记住的版本号
a version number that's higher than the

1021
00:48:26,530 --> 00:48:28,540
高于master服务器记住的版本号
version number the master remembers the

1022
00:48:28,540 --> 00:48:31,600
master服务器假设
master assumes that there was a failure

1023
00:48:31,600 --> 00:48:34,600
当它分配新的primary，
while it was assigning a new primary and

1024
00:48:34,600 --> 00:48:36,760
并采用新的从块服务器获得的更高版本号时
adopts the new the higher version number

1025
00:48:36,760 --> 00:48:38,860
发生了故障
that it heard from a chunk server 

1026
00:48:38,860 --> 00:48:42,250
所以一定是这样
so it must be the case that in order to handle

1027
00:48:42,250 --> 00:48:48,010
为了处理此时的master服务器崩溃
a master crash at this point that 

1028
00:48:48,010 --> 00:48:55,869
master服务器在告诉了primary节点后版本号之后
the master writes its own version number to

1029
00:48:55,869 --> 00:49:02,530
将自己的版本号写入磁盘
disk after telling the primaries there's

1030
00:49:02,530 --> 00:49:03,550
但是这里有一个问题
a bit of a problem here though because

1031
00:49:03,550 --> 00:49:11,880
什么 是不是有ACK
if the was that is there an ACK

1032
00:49:12,410 --> 00:49:17,250
好吧，所以master服务器可能
all right so maybe the master tells the

1033
00:49:17,250 --> 00:49:18,810
告诉primary节点和secondary节点
primaries and backups and that their

1034
00:49:18,810 --> 00:49:20,400
告诉primary节点和secondary节点
primaries and secondaries if they're a

1035
00:49:20,400 --> 00:49:21,720
primary secondary tells him the new

1036
00:49:21,720 --> 00:49:24,450
等待ACK
version number waits for the AK and then

1037
00:49:24,450 --> 00:49:27,870
然后写入磁盘，或对此不满意的操作
writes to disk or something unsatisfying about this 

1038
00:49:27,870 --> 00:49:37,770
我不认为这行得通
I don't believe that works

1039
00:49:37,770 --> 00:49:40,380
因为存在这种可能性
because of the possibility that the

1040
00:49:40,380 --> 00:49:41,940
master服务器重启时
chunk servers with the most recent

1041
00:49:41,940 --> 00:49:44,190
具有最新版本号的块服务器
version numbers being offline at the

1042
00:49:44,190 --> 00:49:46,650
可能是离线的，我们不希望
time the master reboots we wouldn't want

1043
00:49:46,650 --> 00:49:48,360
master服务器不知道当前版本号
the master the master doesn't know the

1044
00:49:48,360 --> 00:49:50,610
master服务器不知道当前版本号
current version number it'll just accept

1045
00:49:50,610 --> 00:49:51,960
它只会接受最高的版本号
whatever highest version number here

1046
00:49:51,960 --> 00:49:54,300
这可能是旧版本号
which could be an old version number 

1047
00:49:54,300 --> 00:49:57,000
好吧，这是我无知的一个领域
alright so this is a an area of my

1048
00:49:57,000 --> 00:49:58,260
我不太明白
ignorance I don't really understand

1049
00:49:58,260 --> 00:50:00,570
master服务器是否首先更新其自己的版本号
whether the master update system version

1050
00:50:00,570 --> 00:50:01,800
然后告诉primary和secondary服务器
number on this first and then tells the

1051
00:50:01,800 --> 00:50:03,600
或是其他方式
primary secondary or the other way around 

1052
00:50:03,600 --> 00:50:06,360
我不确定它采用哪种方式
and I'm not sure it works either

1053
00:50:06,360 --> 00:50:11,340
但无论如何，某种方式
way okay but in any case one way or another 

1054
00:50:11,340 --> 00:50:12,810
主机更新其版本号
the master update is version

1055
00:50:12,810 --> 00:50:14,340
告诉primary, secondary说，看
number tells the primary secondary look

1056
00:50:14,340 --> 00:50:16,140
你的primary和secondary
your primaries and secondaries here's a

1057
00:50:16,140 --> 00:50:17,700
这里是一个新版本号
new version number and so now we have a

1058
00:50:17,700 --> 00:50:19,410
现在我们有了一个可以接受写入的primary
primary which is able to accept writes

1059
00:50:19,410 --> 00:50:21,480
这就是primary的工作
alright that's what the primaries job

1060
00:50:21,480 --> 00:50:23,730
是从客户端获取写入
is to take writes from clients and

1061
00:50:23,730 --> 00:50:26,760
并组织将这些写入执行于各个块服务器
organize applying those writes to the

1062
00:50:26,760 --> 00:50:35,130
并组织将这些写入执行于各个块服务器
various chunk servers and you know the

1063
00:50:35,130 --> 00:50:36,450
版本号的目的是
reason for the version number stuff is

1064
00:50:36,450 --> 00:50:44,270
使master能找到
so that the master will recognize the

1065
00:50:44,420 --> 00:50:49,940
哪些服务器具有最新的块
which servers have this new you know the

1066
00:50:50,240 --> 00:50:52,800
master服务器授予某些块服务器成为primary的能力
master hands out the ability to be

1067
00:50:52,800 --> 00:50:55,320
master服务器授予某些块服务器成为primary的能力
primary for some chunk server we want to

1068
00:50:55,320 --> 00:50:58,950
我们希望能够识别出
be able to recognize if the master

1069
00:50:58,950 --> 00:51:01,260
如果主机崩溃
crashes you know that it was that was

1070
00:51:01,260 --> 00:51:03,840
只有那个primary
the primary that only that primary and

1071
00:51:03,840 --> 00:51:05,070
和secondary
it secondaries which were actually

1072
00:51:05,070 --> 00:51:06,720
负责更新该块
processed which were in charge of

1073
00:51:06,720 --> 00:51:08,250
负责更新该块
updating that chunk that only those

1074
00:51:08,250 --> 00:51:10,530
将来只有那些primary和secondary
primaries and secondaries are allowed to

1075
00:51:10,530 --> 00:51:12,630
可以在未来成为块服务器
be chunk servers in the future and the

1076
00:51:12,630 --> 00:51:14,070
master实现这个目的的方式是
way the master does this is with this

1077
00:51:14,070 --> 00:51:17,270
使用版本号的逻辑
version number logic

1078
00:51:17,480 --> 00:51:21,500
master告诉primary和secondary，
okay so the master tells the primaries

1079
00:51:21,500 --> 00:51:23,119
他们被允许修改此块
and secondaries that there it they're

1080
00:51:23,119 --> 00:51:24,740
他们被允许修改此块
allowed to modify this block 

1081
00:51:24,740 --> 00:51:27,530
它还给primary一个租约
it also gives the primary a lease which

1082
00:51:27,530 --> 00:51:29,390
告诉primary，看
basically tells the primary look you're

1083
00:51:29,390 --> 00:51:31,099
在接下来的60秒内你将是primary
allowed to be primary for the next sixty

1084
00:51:31,099 --> 00:51:33,200
六十秒后，你必须停止
seconds after sixty Seconds you have to

1085
00:51:33,200 --> 00:51:37,280
这是机制的一部分
stop and this is part of the machinery

1086
00:51:37,280 --> 00:51:39,290
确保我们不会有两个primary
for making sure that we don't end up

1087
00:51:39,290 --> 00:51:41,869
确保我们不会有两个primary，我待会再谈
with two primaries I'll talk about a bit later 

1088
00:51:41,869 --> 00:51:46,339
好吧，现在假设我们是primary
okay so now we were primary now

1089
00:51:46,339 --> 00:51:50,089
现在master告诉client
the master tells the client who the

1090
00:51:50,089 --> 00:51:54,440
谁是primary，谁是secondary
primary and the secondaries are and at

1091
00:51:54,440 --> 00:51:59,050
此时，我们正在执行
this point we're we're executing in

1092
00:51:59,050 --> 00:52:02,240
论文中的图二
figure two in the paper the client now

1093
00:52:02,240 --> 00:52:04,040
client现在知道谁是primary, 谁是secondary
knows who the primary secondaries are 

1094
00:52:04,040 --> 00:52:05,660
以某种秩序，这篇论文
in some order or another and the paper

1095
00:52:05,660 --> 00:52:08,180
解释了一种聪明的方法来管理
explains a sort of clever way to manage this 

1096
00:52:08,180 --> 00:52:10,849
以某种秩序
in some order or another the client

1097
00:52:10,849 --> 00:52:13,250
client端将要追加的数据副本
sends a copy of the data it wants to be

1098
00:52:13,250 --> 00:52:15,230
发送给primary
appended to the primary and all the

1099
00:52:15,230 --> 00:52:18,440
和所有secondary，并且primary和
secondaries and the primary and the

1100
00:52:18,440 --> 00:52:20,390
secondary将该数据写入一个临时位置
secondaries write that data to a

1101
00:52:20,390 --> 00:52:22,099
它尚未附加到文件中
temporary location it's not appended to

1102
00:52:22,099 --> 00:52:24,380
在它们都说yes
the file yet after they've all said yes

1103
00:52:24,380 --> 00:52:29,180
我们有数据之后
we have the data the client sends a

1104
00:52:29,180 --> 00:52:31,130
client发送一条消息到primary说，看
message to the primary saying look you

1105
00:52:31,130 --> 00:52:33,470
你和所有secondary节点都有
know you and all the secondaries have

1106
00:52:33,470 --> 00:52:35,569
我要追加到此文件的数据
the data I'd like to append it for this

1107
00:52:35,569 --> 00:52:36,579
file

1108
00:52:36,579 --> 00:52:38,960
primary可能正在同时
the primary maybe is receiving these

1109
00:52:38,960 --> 00:52:40,520
从许多不同的client端接收这些请求
requests from lots of different clients

1110
00:52:40,520 --> 00:52:43,010
它选择某种顺序
concurrently it picks some order execute

1111
00:52:43,010 --> 00:52:45,260
一次执行一个client请求
the client request one at a time and for

1112
00:52:45,260 --> 00:52:48,260
一次执行一个client请求
each client appends request 

1113
00:52:48,260 --> 00:52:50,450
primary查看文件末尾的偏移量
the primary looks at the offset that's the end of

1114
00:52:50,450 --> 00:52:53,030
当前块的当前末尾
the file the current end of the current chunk 

1115
00:52:53,030 --> 00:52:54,740
确保块中有足够的剩余空间
makes sure there's enough

1116
00:52:54,740 --> 00:52:56,480
确保块中有足够的剩余空间
remaining space in the chunk and 

1117
00:52:56,480 --> 00:52:59,960
然后将client记录
then tells then writes the clients record to

1118
00:52:59,960 --> 00:53:02,240
写入当前块的末尾
the end of the current chunk and tells

1119
00:53:02,240 --> 00:53:04,369
并告诉所有secondary服务器
all the secondaries to also write the

1120
00:53:04,369 --> 00:53:08,359
也将client来的数据写到
clients data to the end to the same

1121
00:53:08,359 --> 00:53:12,010
末尾的相同偏移量处
offset the same offset in their chunks

1122
00:53:12,010 --> 00:53:20,500
primary选一个偏移量
all right so the primary picks an offset

1123
00:53:20,500 --> 00:53:26,480
包括primary的所有备份
all the replicas including the primary

1124
00:53:26,480 --> 00:53:29,180
被指示在偏移处写入新的附加记录
are told to write

1125
00:53:29,180 --> 00:53:36,090
被指示在偏移处写入新的附加记录
the new appended record at at offset 

1126
00:53:36,090 --> 00:53:38,700
这些secondary，它们可能会做，也可能不会
the secondary's they may do it they may not

1127
00:53:38,700 --> 00:53:41,250
可能空间不足
do it maybe run out of space maybe

1128
00:53:41,250 --> 00:53:42,810
可能它们崩溃了
they crashed maybe the network message

1129
00:53:42,810 --> 00:53:45,480
可能从primary来的网络消息丢失了
was lost from the primary 

1130
00:53:45,480 --> 00:53:47,970
因此，如果secondary节点真的以该偏移量
so if a secondary actually wrote the data to its

1131
00:53:47,970 --> 00:53:50,760
将数据写入其磁盘，它将回复“yes”给primary
disk at that offset it will reply yes to

1132
00:53:50,760 --> 00:53:52,859
如果primary从所有secondary
the primary if the primary collects a

1133
00:53:52,859 --> 00:53:57,740
都收集到yes的回复
yes answer from all of the secondaries

1134
00:53:58,520 --> 00:54:02,190
如果他们都真的写出
so if they all of all of them managed to

1135
00:54:02,190 --> 00:54:03,630
并回复primary说
actually write and reply to the primary

1136
00:54:03,630 --> 00:54:08,250
yes我做到了
saying yes I did it then the primary is

1137
00:54:08,250 --> 00:54:10,800
那么primary服务器将向client端回复“success”
going to reply reply success to the client 

1138
00:54:10,800 --> 00:54:18,930
如果primary没有收到
if the primary doesn't get an

1139
00:54:18,930 --> 00:54:21,510
某一个secondary的回复
answer from one of the secondaries 

1140
00:54:21,510 --> 00:54:23,580
或某个secondary回复，sorry发生了一些不好的事情
or the secondary reply sorry something bad

1141
00:54:23,580 --> 00:54:25,590
我磁盘空间不足，我的磁盘挂了
happened I ran out of disk space my disk died

1142
00:54:25,590 --> 00:54:28,980
然后primary
I don't know what, then the primary

1143
00:54:28,980 --> 00:54:37,950
会回复“no”给client
replies no to the client and the paper

1144
00:54:37,950 --> 00:54:39,420
论文说，如果client从primary得到像这种的错误
says oh if the client gets an error like

1145
00:54:39,420 --> 00:54:42,000
论文说，如果client从primary得到像这种的错误
that back in the primary the client is

1146
00:54:42,000 --> 00:54:44,369
client应该重新发起整个追加的过程
supposed to reissue the entire append sequence 

1147
00:54:44,369 --> 00:54:46,020
重新开始，与master通信
starting again talking to the

1148
00:54:46,020 --> 00:54:48,530
以找出文件末尾
master to find out the most grease the

1149
00:54:48,530 --> 00:54:50,369
以找出文件末尾
chunk at the end of the file

1150
00:54:50,369 --> 00:54:52,500
我想知道client端应该
I want to know the client supposed to

1151
00:54:52,500 --> 00:54:54,300
重新发起整个记录追加操作
reissue the whole record append

1152
00:54:54,300 --> 00:55:01,650
你可能会想，但他们不
operation ah you would think but they

1153
00:55:01,650 --> 00:55:05,180
所以问题是
don't so the question is jeez you know

1154
00:55:05,180 --> 00:55:08,220
primary服务器告诉所有副本
the the primary tells all the replicas

1155
00:55:08,220 --> 00:55:09,869
执行追加操作，也许其中一些服务器成功了
to do the append yeah maybe some of them

1156
00:55:09,869 --> 00:55:10,830
有些没有成功
do some of them don't

1157
00:55:10,830 --> 00:55:12,869
如果其中一些没有成功
right if some of them don't then we

1158
00:55:12,869 --> 00:55:14,460
那么我们会向client端报错误
apply an error to the client

1159
00:55:14,460 --> 00:55:16,109
因此client认为，追加没有发生
so the client thinks oh the append didn't happen

1160
00:55:16,109 --> 00:55:18,630
但是追加成功的其他副本
but those other replicas where the append

1161
00:55:18,630 --> 00:55:23,550
确实追加成功了
succeeded they did append so now we have

1162
00:55:23,550 --> 00:55:25,400
所以现在我们有不数据的备份
replicas donor the same data one of them

1163
00:55:25,400 --> 00:55:27,480
返回错误的那个没有执行追加
the one that returned in error didn't do

1164
00:55:27,480 --> 00:55:28,890
返回的“yes”的那些服务器
the append and the ones they returned

1165
00:55:28,890 --> 00:55:31,830
确实做了追加
yes did do the append so that is just

1166
00:55:31,830 --> 00:55:35,119
这就是GFS运作的方式
the way GFS works

1167
00:55:44,590 --> 00:55:47,590
如果一个读者随后读取了此文件
yeah so if a reader then reads this file

1168
00:55:47,590 --> 00:55:50,330
取决于哪个备份
they depending on what replica they be

1169
00:55:50,330 --> 00:55:53,360
他们可能会看到附加的记录
they may either see the appended record

1170
00:55:53,360 --> 00:55:56,810
或者可能看不到，如果记录追加失败
or they may not if the record append

1171
00:55:56,810 --> 00:55:59,120
但是如果记录追加成功
but if the record append succeeded if

1172
00:55:59,120 --> 00:56:00,920
如果client收到“success”成功消息
the client got a success message back

1173
00:56:00,920 --> 00:56:03,920
那么这意味着所有副本
then that means all of the replicas

1174
00:56:03,920 --> 00:56:05,420
都以相同的偏移量追加了该记录
appended that record at the same offset

1175
00:56:05,420 --> 00:56:10,160
如果client得到“no”的回复
if the client gets a no back then zero

1176
00:56:10,160 --> 00:56:14,090
那么零个或多个副本
or more of the replicas may have

1177
00:56:14,090 --> 00:56:15,740
可能已经追加了该偏移量的记录
appended the record of that all set and

1178
00:56:15,740 --> 00:56:20,240
而其他副本则没有，所以client得到了“No”
the other ones not so the client got a No 

1179
00:56:20,240 --> 00:56:22,280
那么这意味着
then that means that some replicas

1180
00:56:22,280 --> 00:56:25,130
也许有些备份有记录，有些则没有
maybe some replicas have the record and some don't 

1181
00:56:25,130 --> 00:56:27,860
所以你读取的东西
so what you which were

1182
00:56:27,860 --> 00:56:29,750
你可能会看到记录
roughly read from you know you may or

1183
00:56:29,750 --> 00:56:32,980
也可能不会看到
may not see the record yeah

1184
00:56:39,410 --> 00:56:45,319
所有secondary文件都看到相同的版本号
oh that all the replicas are the same

1185
00:56:45,319 --> 00:56:47,240
所有secondary文件都看到相同的版本号
all the secondaries are the same version

1186
00:56:47,240 --> 00:56:49,430
版本号仅在
number so the version number only

1187
00:56:49,430 --> 00:56:51,500
master服务器分配新的primary服务器时更改
changes when the master assigns a new

1188
00:56:51,500 --> 00:56:53,900
这可能仅在primary出故障时
primary which would ordinarily happen

1189
00:56:53,900 --> 00:56:55,309
才会发生
and probably only happen if the primary failed 

1190
00:56:55,309 --> 00:56:58,270
所以我们在谈论的是
so what we're talking about is is

1191
00:56:58,270 --> 00:57:00,200
具有新版本号的副本
replicas that have the fresh version number 

1192
00:57:00,200 --> 00:57:02,660
你无法分辨出
alright and you can't tell from

1193
00:57:02,660 --> 00:57:03,740
这些副本
looking at them that they're missing

1194
00:57:03,740 --> 00:57:08,059
是不同的
that the replicas are different but

1195
00:57:08,059 --> 00:57:09,319
但可能他们就是不同的
maybe they're different and 

1196
00:57:09,319 --> 00:57:11,390
这样做的理由是
the justification for this is that yeah you

1197
00:57:11,390 --> 00:57:13,160
可能不是所有备份
know maybe the replicas don't all have

1198
00:57:13,160 --> 00:57:16,099
都具有那个追加记录
that the appended record but that's the

1199
00:57:16,099 --> 00:57:18,200
但是在这种情况下，primary回答是“No”
case in which the primary answer no to

1200
00:57:18,200 --> 00:57:20,180
并且client端知道
the clients and the client knows that

1201
00:57:20,180 --> 00:57:22,940
写入失败了
the write failed and the reasoning

1202
00:57:22,940 --> 00:57:24,410
这样做背后的原因是，client端
behind this is that then the client

1203
00:57:24,410 --> 00:57:27,859
将重新发布该追加
library will reissue the append 

1204
00:57:27,859 --> 00:57:29,480
这样追加的记录就会出现
so the appended record will show up you know

1205
00:57:29,480 --> 00:57:33,260
最终附加会成功
eventually the append succeed you

1206
00:57:33,260 --> 00:57:36,920
因为client端，会继续
would think because the client I'll keep

1207
00:57:36,920 --> 00:57:38,480
发出（写请求）直到成功，然后
reissuing it until succeeds and then

1208
00:57:38,480 --> 00:57:39,770
当它成功时，这意味着
when it succeeds that means there's

1209
00:57:39,770 --> 00:57:41,510
会有更大的偏移量
gonna be some offset you know farther on

1210
00:57:41,510 --> 00:57:43,460
该记录实际上
the file where that record actually

1211
00:57:43,460 --> 00:57:45,859
出现在所有副本中的这个偏移量处
occurs in all the replicas as well as

1212
00:57:45,859 --> 00:57:48,049
而那之前的偏移量处
offsets preceding that word only occurs

1213
00:57:48,049 --> 00:57:52,690
追加的记录仅在少数备份中出现
in a few of the replicas yes

1214
00:58:04,680 --> 00:58:11,779
哦，这是一个好问题
oh this is a great question

1215
00:58:11,779 --> 00:58:15,690
对于基础网络而言
the exact path that the right data takes

1216
00:58:15,690 --> 00:58:17,910
数据所采用的确切路径
might be quite important with respect to

1217
00:58:17,910 --> 00:58:19,410
可能非常重要
the underlying network and the paper

1218
00:58:19,410 --> 00:58:22,950
而且论文里说，即使
somewhere says even though when the

1219
00:58:22,950 --> 00:58:24,539
当论文第一次谈论它时
paper first talks about it he claims

1220
00:58:24,539 --> 00:58:26,490
他声称client将数据发送到每个副本
that the client sends the data to each

1221
00:58:26,490 --> 00:58:29,309
实际上
replica in fact later on it changes the

1222
00:58:29,309 --> 00:58:31,289
稍后它改变了说法
tune and says the client sends it to

1223
00:58:31,289 --> 00:58:33,539
说client端将数据仅发送到最近的副本
only the closest of the replicas and

1224
00:58:33,539 --> 00:58:36,359
然后该副本
then the replicas then that replica

1225
00:58:36,359 --> 00:58:37,829
将数据转发到另一个副本 
forwards the data to another replica

1226
00:58:37,829 --> 00:58:39,630
沿着某种链条
along I sort of chained until all the

1227
00:58:39,630 --> 00:58:41,940
直到所有的副本都拿到数据
replicas had the data and that path of

1228
00:58:41,940 --> 00:58:43,770
并且该链的路径用于最小化
that chain is taken to sort of minimize

1229
00:58:43,770 --> 00:58:46,859
跨越数据中心的
crossing bottleneck inter switch links

1230
00:58:46,859 --> 00:59:00,390
瓶颈内部交换机链接
in a data center yes the version number

1231
00:59:00,390 --> 00:59:03,539
版本号只在master服务器
only gets incremented if the master

1232
00:59:03,539 --> 00:59:06,119
认为没有primary时增加
thinks there's no primary so it's a so

1233
00:59:06,119 --> 00:59:09,359
在通常的流程中
in the ordinary sequence there already

1234
00:59:09,359 --> 00:59:13,710
那个chunk已经有一个primary
be a primary for that chunk the the

1235
00:59:13,710 --> 00:59:16,680
master会记得
the the master sort of will remember oh

1236
00:59:16,680 --> 00:59:18,180
该块已经有一个primary和secondary
gosh there's already a primary and

1237
00:59:18,180 --> 00:59:19,470
该块已经有一个primary和secondary
secondary for that chunk and it'll just

1238
00:59:19,470 --> 00:59:20,640
就不会进行这个master挑选（primary的过程）
it won't go through this master

1239
00:59:20,640 --> 00:59:22,079
不会增加版本号
selection it won't increment the version

1240
00:59:22,079 --> 00:59:24,450
它只会告诉client端，看
number it'll just tell the client look

1241
00:59:24,450 --> 00:59:26,400
这里是primary
up here's the primary with with no

1242
00:59:26,400 --> 00:59:29,270
没有发生版本号更改
version number change

1243
00:59:42,340 --> 00:59:47,090
我的理解是，如果这是
my understanding is that if this is this

1244
00:59:47,090 --> 00:59:49,130
我想你是在问一个有趣的问题
I think you're asking a you're asking an

1245
00:59:49,130 --> 00:59:51,050
我想你是在问一个有趣的问题
interesting question so in this scenario

1246
00:59:51,050 --> 00:59:52,940
在这种情况下，primary向client端
in which the primaries answered

1247
00:59:52,940 --> 00:59:54,590
回复失败消息
failure to the client you might think

1248
00:59:54,590 --> 00:59:56,000
你可能会认为一定哪里出错了
something must be wrong with something

1249
00:59:56,000 --> 00:59:57,860
应该在继续进行之前将其修复
and that it should be fixed before you proceed 

1250
00:59:57,860 --> 00:59:59,870
事实上，据我所知
in fact as far as I can tell the

1251
00:59:59,870 --> 01:00:03,320
论文里没有立即
paper there's no immediate anything the

1252
01:00:03,320 --> 01:00:08,300
client尝试重试追加内容
client retries the append you know

1253
01:00:08,300 --> 01:00:10,010
因为也许问题是网络消息丢失了
because maybe the problem was a network

1254
01:00:10,010 --> 01:00:11,570
所以没有什么可以修复的
message got lost so there's nothing to repair 

1255
01:00:11,570 --> 01:00:12,980
本该传递出去的网络消息丢失了
right you know the network

1256
01:00:12,980 --> 01:00:13,850
本该传递出去的网络消息丢失了
message got lost we should be

1257
01:00:13,850 --> 01:00:15,080
这是重传网络消息的一种复杂方式
transmitted and this is sort of a

1258
01:00:15,080 --> 01:00:17,600
这是重传网络消息的一种复杂方式
complicated way of retransmitting the

1259
01:00:17,600 --> 01:00:19,040
也许那是最常见的失败类型
network message maybe that's the most

1260
01:00:19,040 --> 01:00:21,020
也许那是最常见的失败类型
common kind of failure in that case just

1261
01:00:21,020 --> 01:00:22,790
我们什么都不用改变
we don't change anything it's still the

1262
01:00:22,790 --> 01:00:26,750
相同的primary，相同的secondary
same primary same secondaries the client

1263
01:00:26,750 --> 01:00:28,130
client端重试
retries maybe this time it'll work

1264
01:00:28,130 --> 01:00:29,270
也许这次可以
because the network doesn't

1265
01:00:29,270 --> 01:00:31,490
因为（这次）网络没有丢弃消息
discard a message it's an interesting

1266
01:00:31,490 --> 01:00:32,900
这是一个有趣的问题，这里出问题的是
question though that if what went wrong

1267
01:00:32,900 --> 01:00:35,510
secondary中的一个
here is that one of that there was a

1268
01:00:35,510 --> 01:00:37,910
出了严重的错误或故障
serious error or Fault in one of the secondaries 

1269
01:00:37,910 --> 01:00:41,150
我们想要的是
what we would like is for

1270
01:00:41,150 --> 01:00:43,880
master服务器重新配置那组副本
the master to reconfigure that set of

1271
01:00:43,880 --> 01:00:46,820
丢掉不工作的secondary服务器
replicas to drop that secondary that's

1272
01:00:46,820 --> 01:00:49,460
丢掉不工作的secondary服务器
not working and it would then 

1273
01:00:49,460 --> 01:00:50,900
因为它在执行此代码路径时选择了新的primary
because it's choosing a new primary in executing

1274
01:00:50,900 --> 01:00:52,610
然后master
this code path the master would then

1275
01:00:52,610 --> 01:00:54,890
会增加版本号
increment the version and then we have a

1276
01:00:54,890 --> 01:00:56,750
然后我们有一个新的primary和新的正常工作的secondary
new primary and new working secondaries

1277
01:00:56,750 --> 01:01:00,170
以及一个新版本，还有一个不太好的secondary服务器
with a new version and this not-so-great

1278
01:01:00,170 --> 01:01:02,720
其中有一个旧版本
secondary with an old version and a

1279
01:01:02,720 --> 01:01:04,160
和一个过时的数据副本
stale copy of the data but because that

1280
01:01:04,160 --> 01:01:07,000
但是因为它具有旧版本号
has an old version the master will never

1281
01:01:07,000 --> 01:01:09,260
所以master服务器永远不会将其误认为新版本
never mistake it for being fresh 

1282
01:01:09,260 --> 01:01:10,640
但是论文中没有证据表明
but there's no evidence in the paper that

1283
01:01:10,640 --> 01:01:12,470
那会立即发生
that happens immediately as far as

1284
01:01:12,470 --> 01:01:15,110
论文上说，client只是重试
what's said in the paper the client just retries 

1285
01:01:15,110 --> 01:01:17,180
并希望它稍后能正常工作
and hopes it works again later

1286
01:01:17,180 --> 01:01:19,610
最终master服务器会
eventually the master will 

1287
01:01:19,610 --> 01:01:21,230
如果secondary死了
if the secondary is dead

1288
01:01:21,230 --> 01:01:23,990
最终master会ping所有的块服务器
eventually the master does ping all the

1289
01:01:23,990 --> 01:01:25,850
master将意识到
trunk servers will realize that and will

1290
01:01:25,850 --> 01:01:30,770
并可能会更改
probably then change the set of

1291
01:01:30,770 --> 01:01:32,090
primary和secondary的集合
primaries and secondaries and increment

1292
01:01:32,090 --> 01:01:35,590
并增加版本号，但仅在（重试）以后
the version but only only later

1293
01:01:40,380 --> 01:01:45,660
租约是解决这个问题的答案
the lease the leases that the answer to

1294
01:01:45,660 --> 01:01:49,890
如果master认为
the question what if the master thinks

1295
01:01:49,890 --> 01:01:52,500
primary已经死了怎么办，因为它无法访问到它
the primary is dead because it can't reach it 

1296
01:01:52,500 --> 01:01:53,790
假设我们处于一种情况
right that's supposing we're in a situation 

1297
01:01:53,790 --> 01:01:55,470
在某个时候
where at some point 

1298
01:01:55,470 --> 01:01:58,110
master说，你是primary
the master said you're the primary and the

1299
01:01:58,110 --> 01:01:59,940
master对它们定期执行ping操作
master was like pinging them all the

1300
01:01:59,940 --> 01:02:01,260
检查服务器是否还活着
service periodically to see if they're alive 

1301
01:02:01,260 --> 01:02:02,610
因为如果他们死了
because if they're dead and wants

1302
01:02:02,610 --> 01:02:05,160
想选一个新的primary
to pick a new primary the master sends

1303
01:02:05,160 --> 01:02:07,080
master会向你发送一些ping消息
some pings to you you're the primary and

1304
01:02:07,080 --> 01:02:09,690
你是primary，你没有回应
you don't respond right so you would

1305
01:02:09,690 --> 01:02:11,850
在那个时候，你会想
think that at that point where gosh

1306
01:02:11,850 --> 01:02:14,060
你没有回应我的ping
you're not responding to my pings 

1307
01:02:14,060 --> 01:02:16,560
那么你可能会认为那时的master
then you might think the master at that point

1308
01:02:16,560 --> 01:02:20,790
会指定一个新的primary
would designate a new primary 

1309
01:02:20,790 --> 01:02:23,820
事实证明，这本身就是一个错误
it turns out that by itself is a mistake and 

1310
01:02:23,820 --> 01:02:26,130
原因是，为什么
the reason for that， the reason why it's a

1311
01:02:26,130 --> 01:02:30,090
采用那个简单的设计
mistake to do that simple did you know

1312
01:02:30,090 --> 01:02:32,400
是一个错误的原因是
use that simple design is that 

1313
01:02:32,400 --> 01:02:33,870
我可能会ping你
I may be pinging you and the reason why I'm not

1314
01:02:33,870 --> 01:02:35,400
而我没有得到回复的原因是因为
getting responses is because then

1315
01:02:35,400 --> 01:02:36,570
我和你之间的网络有问题
there's something wrong with a network

1316
01:02:36,570 --> 01:02:38,190
我和你之间的网络有问题
between me and you so there's a

1317
01:02:38,190 --> 01:02:39,870
所以有可能你还活着
possibility that you're alive you're the

1318
01:02:39,870 --> 01:02:41,220
你是primary，你还活着，我ping你
primary you're alive I'm pinging you the

1319
01:02:41,220 --> 01:02:42,750
网络丢弃了那个的数据包
network is dropping that packets 

1320
01:02:42,750 --> 01:02:44,280
但你可以与其他client交谈
but you can talk to other clients and you're

1321
01:02:44,280 --> 01:02:46,320
并且正在处理其他client的请求
serving requests from other clients you know 

1322
01:02:46,320 --> 01:02:49,140
如果我，master
and if I if I the master sort of

1323
01:02:49,140 --> 01:02:51,840
为那个块指定了​​一个新的primary
designated a new primary for that chunk

1324
01:02:51,840 --> 01:02:54,600
现在我们有两个primary处理写操作
now we'd have two primaries processing writes 

1325
01:02:54,600 --> 01:02:56,340
但是有两个不同的数据拷贝
but two different copies of the data 

1326
01:02:56,340 --> 01:02:58,830
我们会有完全分叉的数据
and so now we have totally

1327
01:02:58,830 --> 01:03:02,370
我们会有完全分叉的数据
diverging copies the data and 

1328
01:03:02,370 --> 01:03:07,560
这种具有两个primary处理请求
that's called that error having two primaries

1329
01:03:07,560 --> 01:03:10,770
且不知道彼此的错误
or whatever processing requests without

1330
01:03:10,770 --> 01:03:12,570
称为脑裂
knowing each other it's called split brain 

1331
01:03:12,570 --> 01:03:16,710
我在黑板上写出这个
and I'm writing this on board

1332
01:03:16,710 --> 01:03:19,440
因为这是一个重要的想法
because it's an important idea and 

1333
01:03:19,440 --> 01:03:23,160
它会再次出现
it'll come up again and it's caused or 

1334
01:03:23,160 --> 01:03:24,540
这通常是由网络分区引起的
it's usually said to be caused by network

1335
01:03:24,540 --> 01:03:33,120
这是某种网络错误，
partition that is some network error in

1336
01:03:33,120 --> 01:03:34,260
其中master服务器无法与primary服务器通信
which the master can't talk to the

1337
01:03:34,260 --> 01:03:35,640
但primary服务器可以与client端通信
primary but the primary can talk to

1338
01:03:35,640 --> 01:03:38,330
某种部分网络故障
clients sort of partial network failure

1339
01:03:38,330 --> 01:03:41,160
这些是构建这类存储系统中最难处理的问题之一
and you know these are some of the these

1340
01:03:41,160 --> 01:03:44,760
这些是构建这类存储系统中最难处理的问题之一
are the hardest problems to deal with

1341
01:03:44,760 --> 01:03:46,470
这些是构建这类存储系统中最难处理的问题之一
and building these kind of storage systems 

1342
01:03:46,470 --> 01:03:49,170
好，那是问题所在
okay so that's the problem is 

1343
01:03:49,170 --> 01:03:51,690
我们想排除
we want to rule out the possibility of

1344
01:03:51,690 --> 01:03:54,280
为同一块错误地
mistakingly designating two primaries

1345
01:03:54,280 --> 01:03:56,210
指定两个primary的可能性
for the same chunk 

1346
01:03:56,210 --> 01:03:58,610
master为之采取的方式是
the way the master achieves that is that 

1347
01:03:58,610 --> 01:04:00,920
当它指定一个primary时
when it designates a primary it says it gives a

1348
01:04:00,920 --> 01:04:03,320
它给primary提供租约
primary a lease which is basically the

1349
01:04:03,320 --> 01:04:05,590
租约是在一定时间内作为primary的权利
right to be primary until a certain time

1350
01:04:05,590 --> 01:04:08,990
master记得
the master knows it remembers and knows

1351
01:04:08,990 --> 01:04:12,500
租约持续多长时间
how long the least lasts and 

1352
01:04:12,500 --> 01:04:14,960
并且primary知道租约能持续多久
the primary knows how long is lease lasts 

1353
01:04:14,960 --> 01:04:18,800
如果租约到期，primary服务器知道它已到期
if the lease expires the primary knows that it

1354
01:04:18,800 --> 01:04:20,570
将停止执行client端请求
expires and will simply stop executing

1355
01:04:20,570 --> 01:04:23,150
租约到期后
client requests it'll ignore or reject

1356
01:04:23,150 --> 01:04:24,830
它将忽略或拒绝client请求
client requests after the lease expired

1357
01:04:24,830 --> 01:04:27,800
因此，如果master不能
and therefore if the master can't talk

1358
01:04:27,800 --> 01:04:29,570
与primary通信，master将会
to the primary and the master would like

1359
01:04:29,570 --> 01:04:31,220
指定一个新的primary
to designate a new primary 

1360
01:04:31,220 --> 01:04:33,830
master必须要等待
the master must wait for the lease to expire for

1361
01:04:33,830 --> 01:04:35,270
上一个primary的租约到期
the previous primary so that means

1362
01:04:35,270 --> 01:04:37,670
因此，这意味着master将
master is going to sit on its hands for

1363
01:04:37,670 --> 01:04:40,010
坐下来等待一个60秒的租约周期
one lease period 60 seconds 

1364
01:04:40,010 --> 01:04:41,660
之后，可以确信旧的primary服务器
after that it's guaranteed the old primary will

1365
01:04:41,660 --> 01:04:44,510
将停止其primary角色
stop operating its primary and now 

1366
01:04:44,510 --> 01:04:46,160
现在，master可以安全地指定新的primary
the master can safely designate a new

1367
01:04:46,160 --> 01:04:50,810
而不会产生这种可怕的裂脑情况
primary without producing this terrible

1368
01:04:50,810 --> 01:04:54,460
而不会产生这种可怕的裂脑情况
split brain situation

1369
01:05:02,299 --> 01:05:14,119
问题是说，为什么指定一个新的primary
oh so the question is why is designated

1370
01:05:14,119 --> 01:05:15,920
是坏的（设计） 
a new primary bad since the clients

1371
01:05:15,920 --> 01:05:18,079
既然client总是先问master
always ask the master first and 

1372
01:05:18,079 --> 01:05:20,059
所以master改变主意
so the master changes its mind then subsequent

1373
01:05:20,059 --> 01:05:22,819
然后后续将client定向到新的primary
clients will direct the clients to the new primary 

1374
01:05:22,819 --> 01:05:26,390
原因之一是
well one reason is that the

1375
01:05:26,390 --> 01:05:28,429
client端缓存以提高效率
clients cache for efficiency the clients

1376
01:05:28,429 --> 01:05:31,279
client至少在短时间内
cache the identity of the primary for at

1377
01:05:31,279 --> 01:05:34,009
缓存primary的身份信息
least for short periods of time 

1378
01:05:34,009 --> 01:05:37,489
即使他们没有（缓存），一个糟糕的事件序列是
even if they didn't though the bad sequence is

1379
01:05:37,489 --> 01:05:40,640
我是master，你问我
that I'm the prime the master you ask me

1380
01:05:40,640 --> 01:05:43,449
primary是谁，我向你发送一条消息
who the primary is I send you a message

1381
01:05:43,449 --> 01:05:46,369
说primary是服务器1
saying the primary is server one right

1382
01:05:46,369 --> 01:05:47,809
该消息在网络中传递
and that message is inflate in the network 

1383
01:05:47,809 --> 01:05:50,630、
我是master
and then I'm the master I you know 

1384
01:05:50,630 --> 01:05:52,160
我认为有人失败了
I think somebody's failed whatever

1385
01:05:52,160 --> 01:05:53,269
我认为primary已经出了故障
I think that primary is filled 

1386
01:05:53,269 --> 01:05:55,219
我指定了新的primary
I designated a new primary and I send the

1387
01:05:55,219 --> 01:05:56,209
我发送给primary信息说，你是primary
primary message saying you're the primary 

1388
01:05:56,209 --> 01:05:57,619
然后我开始回答其他client
and I start answering other

1389
01:05:57,619 --> 01:06:00,349
谁是primary
clients who ask the primary is saying

1390
01:06:00,349 --> 01:06:01,400
比如说那个服务器是primary
that that over there is the primary

1391
01:06:01,400 --> 01:06:03,019
然而给你的讯息仍在传递中
while the message to you is still in flight 

1392
01:06:03,019 --> 01:06:04,880
你收到消息里的旧primary，
you receive the message saying

1393
01:06:04,880 --> 01:06:07,130
你想
the old primaries the primary you think

1394
01:06:07,130 --> 01:06:10,219
天哪，我刚从master那里得到这个
gosh I just got this from the master I'm

1395
01:06:10,219 --> 01:06:11,630
我要去跟那个primary通信
gonna go talk to that primary and

1396
01:06:11,630 --> 01:06:13,459
如果没有一些更聪明的机制
without some much more clever scheme

1397
01:06:13,459 --> 01:06:14,859
你不可能意识到（primary是旧的）
there's no way you could realize that

1398
01:06:14,859 --> 01:06:16,849
即使你刚刚从master那里获得了这些信息
even though you just got this

1399
01:06:16,849 --> 01:06:19,309
即使你刚刚从master那里获得了这些信息
information from the master it's already

1400
01:06:19,309 --> 01:06:21,679
它也已经过时
out of date and if that primary serves

1401
01:06:21,679 --> 01:06:24,410
而且如果该primary接受了你的修改请求
your modification requests now we have

1402
01:06:24,410 --> 01:06:27,920
现在我们必须要回复你“success”
to and and respond success to you right

1403
01:06:27,920 --> 01:06:35,349
那么我们有两个相互冲突的备份
then we have two conflicting replicas

1404
01:06:35,890 --> 01:06:38,890
yes

1405
01:06:41,910 --> 01:06:50,710
请再说一次，你有一个新文件，且没有备份
again you've a new file and no replicas

1406
01:06:50,710 --> 01:06:53,410
好的，如果你有一个新文件，没有备份
okay so if you have a new file no

1407
01:06:53,410 --> 01:06:55,180
甚至是已有文件，没有备份
replicas or even an existing file and no replicas 

1408
01:06:55,180 --> 01:06:58,090
你将会走我在黑板上画的路线
the you'll take the path I drew

1409
01:06:58,090 --> 01:07:00,130
你将会走我在黑板上画的路线
on the blackboard the master will

1410
01:07:00,130 --> 01:07:02,140
master会收到client的要求说
receive a request from a client saying

1411
01:07:02,140 --> 01:07:04,270
我想追加这个文件
oh I'd like to append this file and

1412
01:07:04,270 --> 01:07:06,430
我猜master会首先发现
then well I guess the master will first

1413
01:07:06,430 --> 01:07:08,200
没有任何块与该文件关联
see there's no chunks associated with that file 

1414
01:07:08,200 --> 01:07:11,710
它将创造一个新的
and it will just make up a new

1415
01:07:11,710 --> 01:07:13,570
块标识符
chunk identifier or perhaps by calling

1416
01:07:13,570 --> 01:07:15,730
可能通过调用随机数生成器
the random number generator 

1417
01:07:15,730 --> 01:07:17,920
然后在其块信息表中查看
and then it'll look in its chunk information

1418
01:07:17,920 --> 01:07:20,080
发现，天哪
table and see gosh I don't have any

1419
01:07:20,080 --> 01:07:22,030
我没有关于那块的任何信息
information about that chunk and 

1420
01:07:22,030 --> 01:07:24,730
它会创建一个新记录
it'll make up a new record saying but it must

1421
01:07:24,730 --> 01:07:26,410
但一定是特殊情况代码
be special case code where it says well

1422
01:07:26,410 --> 01:07:28,720
我不知道任何版本号
I don't know any version number 

1423
01:07:28,720 --> 01:07:30,850
这个块不存在，我要
this chunk doesn't exist I'm just gonna make

1424
01:07:30,850 --> 01:07:32,740
创建一个新的版本号1
up a new version number one 

1425
01:07:32,740 --> 01:07:35,380
随机选择一个primary和一组secondary
pick a random primary and set of secondaries

1426
01:07:35,380 --> 01:07:37,900
并告诉它们，
and tell them look you are responsible

1427
01:07:37,900 --> 01:07:40,660
你们对此新的空块负责，请开始工作
for this new empty chunk please get to work 

1428
01:07:40,660 --> 01:07:47,020
论文说默认情况下
the paper says three replicas per

1429
01:07:47,020 --> 01:07:50,110
每个块有三个备份，通常是一个primary
chunk by default so typically a primary

1430
01:07:50,110 --> 01:07:52,710
和两个secondary
and two backups

1431
01:08:03,930 --> 01:08:13,270
也许
okay okay so the maybe the most

1432
01:08:13,270 --> 01:08:16,299
这里最重要的是
important thing here is just to repeat

1433
01:08:16,299 --> 01:08:19,890
重复我们几分钟前的讨论
the discussion we had a few minutes ago

1434
01:08:21,540 --> 01:08:32,140
GFS的有意构建
the intentional construction of GFS 

1435
01:08:32,140 --> 01:08:33,790
我们有这些记录追加
we had these record appends is that 

1436
01:08:33,790 --> 01:08:41,009
如果我们有三个备份
if we have three we have three replicas you

1437
01:08:41,009 --> 01:08:43,779
client可能会发送来
know maybe a client sends in and a

1438
01:08:43,779 --> 01:08:46,719
一个记录A的追加
record appends for record A and 

1439
01:08:46,719 --> 01:08:49,569
所有三个备份，primary和两个secondary
all three replicas or the primary and both of the

1440
01:08:49,569 --> 01:08:52,120
成功追加了数据
secondaries successfully append the data

1441
01:08:52,120 --> 01:08:54,069
那种情况下
the chunks and maybe the first record in

1442
01:08:54,069 --> 01:08:55,689
也许这个块中第一个记录是A
the chunk might be a in that case and

1443
01:08:55,689 --> 01:08:57,930
它们都一致，因为它们都追加了
they all agree because they all did it

1444
01:08:57,930 --> 01:09:00,040
假设另一个client进来说
supposing another client comes in says

1445
01:09:00,040 --> 01:09:03,339
我要追加记录B
look I want append record B 

1446
01:09:03,339 --> 01:09:06,250
但消息传递给某个备份时丢失
but the message is lost to one of the replicas

1447
01:09:06,250 --> 01:09:08,410
因为网络某种错误
the network whatever supposably the

1448
01:09:08,410 --> 01:09:11,589
但另外两个备份拿到了消息
message by mistake but the other two

1449
01:09:11,589 --> 01:09:13,390
但另外两个备份拿到了消息
replicas get the message and 

1450
01:09:13,390 --> 01:09:14,380
其中一个是primary
one of them's a primary and my other

1451
01:09:14,380 --> 01:09:16,000
另一个是secondary，它们都将文件追加
secondaries they both append the file

1452
01:09:16,000 --> 01:09:19,390
所以现在我们有两个备份有记录
so now what we have is two the replicas

1453
01:09:19,390 --> 01:09:21,759
另一个没有任何记录
that B and the other one doesn't have anything 

1454
01:09:21,759 --> 01:09:26,410
然后可能有第三个client
and then may be a third client

1455
01:09:26,410 --> 01:09:29,109
想要追加C
wants to append C and maybe the remember

1456
01:09:29,109 --> 01:09:30,460
记得这是primary
that this is the primary 

1457
01:09:30,460 --> 01:09:32,738
primary选择偏移量，
the primary picks the offset since the primary just

1458
01:09:32,738 --> 01:09:35,109
primary要告诉secondary，看
gonna tell the secondaries look

1459
01:09:35,109 --> 01:09:38,620
这个时候把记录C写在这个块里
write record C at this point in the

1460
01:09:38,620 --> 01:09:43,450
他们都在这里写C
chunk they all right C here 

1461
01:09:43,450 --> 01:09:45,040
对于B的client来说
now the client for B the rule for a client for

1462
01:09:45,040 --> 01:09:47,830
它的请求收到error回复
B that for the client that gets us error

1463
01:09:47,830 --> 01:09:50,439
它的请求收到error回复
back from its request is that 

1464
01:09:50,439 --> 01:09:53,770
它会重新发送请求
it will resend the request so now the client

1465
01:09:53,770 --> 01:09:56,020
因此现在要求追加记录B的客户端
that asked to append record B will ask

1466
01:09:56,020 --> 01:09:57,640
将再次请求追加记录B
again to append record B and 

1467
01:09:57,640 --> 01:10:00,340
这次也许没有网络损失，
this time maybe there's no network losses and all

1468
01:10:00,340 --> 01:10:05,040
所有三个备份追加了记录B
three replicas append record B

1469
01:10:05,040 --> 01:10:07,239
对，这些备份都活着
right and they're all lives there they all

1470
01:10:07,239 --> 01:10:09,870
都拥有最新的版本号
have the most fresh version number and

1471
01:10:09,870 --> 01:10:13,150
如果client读取
now if a client reads

1472
01:10:13,150 --> 01:10:16,830
他们所看到的取决于
what they see depends on the chuck which

1473
01:10:17,820 --> 01:10:20,020
他们看的是哪个备份
replicas they look at it's gonna see in

1474
01:10:20,020 --> 01:10:22,929
它将总共看到所有三个记录
total all three of the records but 

1475
01:10:22,929 --> 01:10:25,030
但它会看到不同的顺序，具体取决于
it'll see in different orders depending on

1476
01:10:25,030 --> 01:10:28,750
读取了哪个备份
which replica reads it'll mean 

1477
01:10:28,750 --> 01:10:31,870
我会看到A B C，然后一个重复的B
I'll see A B C and then a repeat of B so 

1478
01:10:31,870 --> 01:10:33,730
所以如果它读取此备份，将会看到B，然后C
if itreads this replica it'll see B and then C 

1479
01:10:33,730 --> 01:10:36,969
如果它读取这个备份
if it reads this replica it'll see A

1480
01:10:36,969 --> 01:10:39,340
它会看到A，然后在文件填充中留一个空白
and then a blank space in the file

1481
01:10:39,340 --> 01:10:41,920
然后是C然后是B
padding and then C and then B so 

1482
01:10:41,920 --> 01:10:44,199
所以如果你在这里阅读，你会看到C，然后是B
if you read here you see C then B 

1483
01:10:44,199 --> 01:10:47,320
在这里你看到B然后C
if you read here you see B and then C 

1484
01:10:47,320 --> 01:10:49,350
不同的读者会看到不同的结果，
so different readers will see different results and

1485
01:10:49,350 --> 01:10:52,330
也许最糟糕的情况是
maybe the worst situation is it 

1486
01:10:52,330 --> 01:10:54,489
一些client端从primary服务器收到错误
some client gets an error back from the

1487
01:10:54,489 --> 01:10:58,360
因为其中一个secondary
primary because one of the secondaries

1488
01:10:58,360 --> 01:11:00,159
未能执行附加操作
failed to do the append and then 

1489
01:11:00,159 --> 01:11:02,260
然后client端在重新发送请求之前挂掉
the client dies before we sending the

1490
01:11:02,260 --> 01:11:04,030
所以你可能进入这种情形
request so then you might get a

1491
01:11:04,030 --> 01:11:07,030
你的记录D
situation where you have record D

1492
01:11:07,030 --> 01:11:11,890
出现在某些备份中
showing up in some of the replicas and

1493
01:11:11,890 --> 01:11:13,750
而在其他备份则完全没有
completely not showing up anywhere in

1494
01:11:13,750 --> 01:11:16,420
所以你知道在这个方案下，
the other replicas so you know under

1495
01:11:16,420 --> 01:11:19,659
对于primary发送回一个成功的回复的追加操作，
this scheme we have good properties for

1496
01:11:19,659 --> 01:11:23,620
我们有很好的性质
for appends that the primary sent back a successful answer 

1497
01:11:23,620 --> 01:11:26,800
对于primary返回失败的追加操作
for and sort of not so

1498
01:11:26,800 --> 01:11:29,469
对于primary返回失败的追加操作
great properties for appends where the

1499
01:11:29,469 --> 01:11:32,949
就不是那么好的性质了
primary sent back of failure

1500
01:11:32,949 --> 01:11:35,530
并且记录，备份绝对不同
and the records the replicas just absolutely be

1501
01:11:35,530 --> 01:11:37,540
不同的一组备份
different all different sets of replicas

1502
01:11:37,540 --> 01:11:40,440
yes

1503
01:11:44,400 --> 01:11:46,660
我在论文中读到的是
my reading in the paper is that the

1504
01:11:46,660 --> 01:11:49,090
client从流程的最开始就开始
client starts at the very beginning of

1505
01:11:49,090 --> 01:11:51,310
并再次询问master服务器
the process and asked the master again

1506
01:11:51,310 --> 01:11:54,190
该文件中的最后一块是什么
what's the last chunk in this file you know 

1507
01:11:54,190 --> 01:11:55,240
因为如果其他人在文件中追加
because it might be might have

1508
01:11:55,240 --> 01:11:56,710
则可能已更改
changed if other people are pending in

1509
01:11:56,710 --> 01:12:02,820
the file yes

1510
01:12:17,760 --> 01:12:20,290
我不能，我看不懂设计者心里想的
so I can't you know I can't read the

1511
01:12:20,290 --> 01:12:22,720
我观察到的是，
designers mind so the observation is the

1512
01:12:22,720 --> 01:12:24,760
该系统可以设计为
system could have been designed to keep

1513
01:12:24,760 --> 01:12:27,640
使备份保持精确同步
the replicas in precise sync 

1514
01:12:27,640 --> 01:12:30,820
这是真的，你将在实验2和3中实现它
it's absolutely true and you will do it in labs 2 & 3 

1515
01:12:30,820 --> 01:12:33,100
你们将要
so you guys are going to

1516
01:12:33,100 --> 01:12:34,930
设计一个进行备份的系统
design a system that does replication

1517
01:12:34,930 --> 01:12:36,880
该系统真正的使备份保持同步
that actually keeps the replicas in sync

1518
01:12:36,880 --> 01:12:38,490
并且你将了解到
and you'll learn you know there's some

1519
01:12:38,490 --> 01:12:41,020
你需要采取多种技巧
various techniques various things you

1520
01:12:41,020 --> 01:12:43,180
才能做到这一点
have to do in order to do that and 

1521
01:12:43,180 --> 01:12:46,150
其中之一是，
one of them is that there just has to be

1522
01:12:46,150 --> 01:12:47,740
如果你希望备份保持同步，
this rule if you want the replicas to

1523
01:12:47,740 --> 01:12:50,410
就必须遵循的一个规则
stay in sync it has to be this rule that

1524
01:12:50,410 --> 01:12:53,320
必须遵循的规则是你不能将这些部分操作
you can't have these partial operations

1525
01:12:53,320 --> 01:12:54,490
仅应用于某些，而不应用于其他
that are applied to only some and not

1526
01:12:54,490 --> 01:12:56,410
这意味着必须有某种机制来
others and that means that there has to

1527
01:12:56,410 --> 01:12:58,630
这意味着必须有某种机制来
be some mechanism to like where the system 

1528
01:12:58,630 --> 01:13:00,130
即使client死了，
even if the client dies where the

1529
01:13:00,130 --> 01:13:01,900
系统或说，等一下
system says we don't wait a minute 

1530
01:13:01,900 --> 01:13:04,060
有个操作我还没有完成
there was this operation I haven't finished it yet 

1531
01:13:04,060 --> 01:13:07,390
因此，你构建的系统中
so you build systems in which the

1532
01:13:07,390 --> 01:13:11,820
primary实际上要确保备份
primary actually make sure the backups

1533
01:13:11,820 --> 01:13:15,360
获得了每条消息
get every message

1534
01:13:29,460 --> 01:13:34,390
如果第一次写B失败
if the first right B failed you think

1535
01:13:34,390 --> 01:13:37,739
你认为C应该写在B的位置
the C should go with the B

1536
01:13:37,770 --> 01:13:40,450
然而它并没有，你可能认为它应该
well it doesn't you may think it should

1537
01:13:40,450 --> 01:13:42,130
但是这个系统实际运行的方式是
but the way the system actually operates

1538
01:13:42,130 --> 01:13:46,690
primary将C添加到块的末尾，
is that the primary will add C to the

1539
01:13:46,690 --> 01:13:57,730
在B的后面
end of the chunk and the after B yeah I

1540
01:13:57,730 --> 01:13:59,890
这样做的原因之一是
mean one reason for this is that at the

1541
01:13:59,890 --> 01:14:01,480
在写C的时候
time the right for C comes in the

1542
01:14:01,480 --> 01:14:03,010
primary实际上可能不知道B的命运是什么
primary may not actually know what the

1543
01:14:03,010 --> 01:14:05,710
因为我们遇到了多个
fate of B was because we met multiple

1544
01:14:05,710 --> 01:14:07,510
同时请求追加的client
clients submitting appends concurrently

1545
01:14:07,510 --> 01:14:10,600
为了获得高性能
and you know for high performance 

1546
01:14:10,600 --> 01:14:14,890
你希望priamry服务器首先启动B的追加操作
you want the primary to start the append for

1547
01:14:14,890 --> 01:14:17,860
然后一旦我能够，
B first and then as soon as I can got

1548
01:14:17,860 --> 01:14:20,170
就告诉所有人把C写在那个offset之后，
the next stop set tell everybody did you

1549
01:14:20,170 --> 01:14:21,750
这样所有这些事情就可以并行地发生
see so that all this stuff happens in parallel 

1550
01:14:21,750 --> 01:14:25,270
通过减慢速度，
you know by slowing it down you

1551
01:14:25,270 --> 01:14:31,750
primary可以
could you know the primary could sort of

1552
01:14:31,750 --> 01:14:33,760
断定B完全失败了
decide that B it totally failed and 

1553
01:14:33,760 --> 01:14:35,560
然后发送另一轮消息
then send another round of messages saying

1554
01:14:35,560 --> 01:14:39,970
请撤消B的写操作
please undo the write of B and 

1555
01:14:39,970 --> 01:14:43,360
那样会更复杂，更慢
there'll be more complex and slower I'm you know

1556
01:14:43,360 --> 01:14:45,880
同样，这样做的理由是
again the the justification for this is

1557
01:14:45,880 --> 01:14:48,730
这个设计非常简单
that the design is pretty simple it you know 

1558
01:14:48,730 --> 01:14:53,820
它暴露给应用程序一些奇怪之处
it reveals some odd things to applications 

1559
01:14:53,820 --> 01:14:58,060
希望是
and the hope was that

1560
01:14:58,060 --> 01:14:59,680
可以相对容易地编写应用程序，
applications could be relatively easily written 

1561
01:14:59,680 --> 01:15:01,750
去容忍记录的顺序不同或别的
to tolerate records being in

1562
01:15:01,750 --> 01:15:04,960
去容忍记录的顺序不同或别的
different orders or who knows what or 

1563
01:15:04,960 --> 01:15:08,800
如果他们不能，应用程序可以
if they couldn't that applications could

1564
01:15:08,800 --> 01:15:11,080
要么自己安排选择一个顺序
either make their own arrangements for

1565
01:15:11,080 --> 01:15:13,300
要么自己安排选择一个顺序
picking an order themselves and 

1566
01:15:13,300 --> 01:15:14,860
并在文件或其他内容中写入你知道的序列号
writing you know sequence numbers in the files

1567
01:15:14,860 --> 01:15:17,739
or something or you could just have a 

1568
01:15:17,739 --> 01:15:20,140
如果应用程序真的对顺序非常敏感
if application really was very sensitive to

1569
01:15:20,140 --> 01:15:21,910
你就不能有从不同client端到同一文件的并发追加操作
order you could just not have concurrent

1570
01:15:21,910 --> 01:15:24,220
你就不能有从不同client端到同一文件的并发追加操作
depends from different clients to the

1571
01:15:24,220 --> 01:15:27,520
你就不能有从不同client端到同一文件的并发追加操作
same file right you could just you know

1572
01:15:27,520 --> 01:15:29,410
顺序很重要的文件
files where order is very important 

1573
01:15:29,410 --> 01:15:31,390
例如说电影文件
like say it's a movie file you know 

1574
01:15:31,390 --> 01:15:32,750
你不想混杂电影文件中的字节
you don't want to scramble

1575
01:15:32,750 --> 01:15:35,840
你不想混杂电影文件中的字节
bytes in a movie file you just write the

1576
01:15:35,840 --> 01:15:37,550
你就用一个client
Movie file you write the movie to the

1577
01:15:37,550 --> 01:15:40,100
连续地将电影写入文件的
file by one client in sequential order

1578
01:15:40,100 --> 01:15:45,040
而不是并发地追加记录
and not with concurrent record appends

1579
01:15:49,150 --> 01:15:56,680
okay all right

1580
01:15:56,680 --> 01:16:04,400
有人问如何把这种设计
the somebody asked basically what would

1581
01:16:04,400 --> 01:16:06,770
变成一个提供了强大一致性的系统
it take to turn this design into one

1582
01:16:06,770 --> 01:16:08,120
变成一个提供了强大一致性的系统
which actually provided strong

1583
01:16:08,120 --> 01:16:11,960
（它的）一致性更接近
consistency consistency closer to our

1584
01:16:11,960 --> 01:16:13,790
我们的没有任何意外的单服务器模型
sort of single server model where

1585
01:16:13,790 --> 01:16:18,680
我们的没有任何意外的单服务器模型
there's no surprises I don't actually

1586
01:16:18,680 --> 01:16:20,180
我实际上不知道
know because you know that requires an

1587
01:16:20,180 --> 01:16:22,340
因为这需要全新的复杂设计
entire new complex design 

1588
01:16:22,340 --> 01:16:24,560
目前尚不清楚如何将GFS突变为该设计
it's not clear how to mutate GFS to be that design 

1589
01:16:24,560 --> 01:16:26,330
但我可以为你列出一些你需要考虑的事情
but I can list for you lists for you some

1590
01:16:26,330 --> 01:16:27,440
如果你想
things that you would want to think

1591
01:16:27,440 --> 01:16:32,350
将GFS升级到
about if you wanted to upgrade GFS to a

1592
01:16:32,350 --> 01:16:34,460
具有很强的一致性的系统
assistance did have strong consistency

1593
01:16:34,460 --> 01:16:37,370
一个（要考虑的）是你可能
one is that you probably need the

1594
01:16:37,370 --> 01:16:40,940
需要让primary来检测重复的请求
primary to detect duplicate requests so

1595
01:16:40,940 --> 01:16:43,460
这样当第二个B到达
that when this second B comes in the

1596
01:16:43,460 --> 01:16:44,960
primary能够知晓
primary is aware that oh actually you know 

1597
01:16:44,960 --> 01:16:47,030
哦，实际上我们早些时候已看到了该请求
we already saw that request earlier

1598
01:16:47,030 --> 01:16:50,570
并且已经执行了或没有执行
and did it or didn't do it and to try to

1599
01:16:50,570 --> 01:16:52,160
并且要确保B不会在文件中出现两次
make sure that B doesn't show up twice

1600
01:16:52,160 --> 01:16:54,140
所以一个是你将需要重复检测
in the file so one is you're gonna need duplicate detection 

1601
01:16:54,140 --> 01:16:59,570
另一个问题
another issues you

1602
01:16:59,570 --> 01:17:02,660
如果一个secondary执行secondary
probably if a secondary is acting a secondary 

1603
01:17:02,660 --> 01:17:05,000
你真的需要设计系统
you really need to design the system 

1604
01:17:05,000 --> 01:17:06,920
使得，如果primary告诉secondary做某事
so that if the primary tells a

1605
01:17:06,920 --> 01:17:08,180
使得，如果primary告诉secondary做某事
secondary to do something

1606
01:17:08,180 --> 01:17:10,010
secondary真的执行
the secondary actually does it and

1607
01:17:10,010 --> 01:17:12,560
而不仅仅是返回错误
doesn't just return error right 

1608
01:17:12,560 --> 01:17:15,260
对于一个严格一致的系统
for a strictly consistent system having the

1609
01:17:15,260 --> 01:17:16,880
如果secondary能够
secondaries be able to just sort of blow

1610
01:17:16,880 --> 01:17:20,210
终止primary请求
off primary requests with really no

1611
01:17:20,210 --> 01:17:24,170
而无任何代价，这样是不行的 所以我认为
compensation is not okay so I think the

1612
01:17:24,170 --> 01:17:25,730
,secondary必须接受并执行请求
secondaries have to accept requests and execute them 

1613
01:17:25,730 --> 01:17:28,460
或者如果secondary磁盘有某种永久性损坏
or if a secondary has some

1614
01:17:28,460 --> 01:17:30,050
或者如果secondary磁盘有某种永久性损坏
sort of permanent damage like it's disk

1615
01:17:30,050 --> 01:17:32,180
例如错误地拔出了磁盘
got unplugged by mistake this 

1616
01:17:32,180 --> 01:17:34,160
你需要一种机制将secondary从系统中移除
you need to have a mechanism to like take the

1617
01:17:34,160 --> 01:17:36,200
你需要一种机制将secondary从系统中移除
secondary out of the system 

1618
01:17:36,200 --> 01:17:39,140
这样primary可以和剩下的secondary继续工作
so the primary can proceed with the remaining secondaries 

1619
01:17:39,140 --> 01:17:41,750
但这两点GFS都没有做
but GFS kind of doesn't

1620
01:17:41,750 --> 01:17:44,950
至少没有做对
either at least not right away

1621
01:17:45,200 --> 01:17:49,350
所以这也意味着
and so that also means that when the

1622
01:17:49,350 --> 01:17:50,910
当primary的要求secondary追加什么时
when the primary asks secondary's to append something 

1623
01:17:50,910 --> 01:17:52,800
secondary必须小心
the secondaries have to be

1624
01:17:52,800 --> 01:17:54,810
不要将数据暴露给读者
careful not to expose that data to readers 

1625
01:17:54,810 --> 01:17:57,600
直到primary确信
until the primary is sure that

1626
01:17:57,600 --> 01:17:59,250
所有secondary都能够
all the secondaries really will be able

1627
01:17:59,250 --> 01:18:02,610
执行附加操作 因此，你可能
to execute the append so you might need

1628
01:18:02,610 --> 01:18:05,400
需要在写操作有多个阶段
sort of multiple phases in the writes of

1629
01:18:05,400 --> 01:18:06,900
第一阶段，primary请求secondary
first phase in which the primary asks

1630
01:18:06,900 --> 01:18:09,030
我真的想让你执行此操作
the secondaries look you know I really

1631
01:18:09,030 --> 01:18:11,310
你能执行此操作吗
like you to do this operation can you do it 

1632
01:18:11,310 --> 01:18:13,560
但并不是真的执行此操作
but don't don't actually do it yet

1633
01:18:13,560 --> 01:18:15,810
如果所有secondary都答应
and if all the secondaries answer with a

1634
01:18:15,810 --> 01:18:17,670
能够进行操作
promise to be able to do the operation

1635
01:18:17,670 --> 01:18:20,550
只有这个时候，primary说
only then the primary says alright

1636
01:18:20,550 --> 01:18:22,080
好的去吧，每个人都执行你所承诺的操作
everybody go ahead and do that operation

1637
01:18:22,080 --> 01:18:24,570
好的去吧，每个人都执行你所承诺的操作
you promised and people you know 

1638
01:18:24,570 --> 01:18:27,210
这是许多现实世界中
that's the way a lot of real world systems

1639
01:18:27,210 --> 01:18:28,950
强一致的系统的工作方式
strong consistent systems work and that

1640
01:18:28,950 --> 01:18:32,540
这种技巧称为两阶段提交
trick it's called two-phase commit

1641
01:18:32,630 --> 01:18:34,590
另一个问题是，如果primary崩溃
another issue is that if the primary crashes 

1642
01:18:34,590 --> 01:18:38,370
已经有最后的一组
there will have been some last

1643
01:18:38,370 --> 01:18:40,410
primary发给secondary的操作开始了
set of operations that the primary had

1644
01:18:40,410 --> 01:18:44,340
primary发给secondary的操作开始了
launched started to the secondaries 

1645
01:18:44,340 --> 01:18:46,890
但primary在确定所有secondary拿到拷贝之前就崩溃了
but the primary crashed before it was sure

1646
01:18:46,890 --> 01:18:48,900
但primary在确定所有secondary拿到拷贝之前就崩溃了
whether those all the secondaries got

1647
01:18:48,900 --> 01:18:51,660
但primary在确定所有secondary拿到拷贝之前就崩溃了
there copied the operation or not 

1648
01:18:51,660 --> 01:18:54,510
所以如果primary崩溃
so if the primary crashes you know 

1649
01:18:54,510 --> 01:18:56,040
一个新的primary，其中一个secondary
a new primary one of the secondaries is going

1650
01:18:56,040 --> 01:18:57,780
将接任primary
to take over as primary but at that

1651
01:18:57,780 --> 01:19:01,200
但是在那时上，新的primary
point the second the new primary and the

1652
01:19:01,200 --> 01:19:03,240
和其余的secondary
remaining secondaries may differ in the

1653
01:19:03,240 --> 01:19:05,580
在最后的几次操作中可能会有所不同 
last few operations because maybe some

1654
01:19:05,580 --> 01:19:07,200
因为也许其中一些在primary服务器崩溃之前
of them didn't get the message before

1655
01:19:07,200 --> 01:19:09,030
没有收到消息
the primary crashed and so 

1656
01:19:09,030 --> 01:19:11,490
因此，新的primary开始时必须与secondary
the new primer has to start by explicitly

1657
01:19:11,490 --> 01:19:15,300
显式地重新同步
resynchronizing with the secondaries to

1658
01:19:15,300 --> 01:19:17,010
以确保他们的操作历史的尾部是一样的
make sure that the sort of the tail of

1659
01:19:17,010 --> 01:19:20,750
以确保他们的操作历史的尾部是一样的
their operation histories are the same

1660
01:19:21,080 --> 01:19:24,060
最后，为了解决这个问题
finally to deal with this problem of oh

1661
01:19:24,060 --> 01:19:25,530
有时候secondary会有所不同
you know there may be times when the

1662
01:19:25,530 --> 01:19:28,500
或client可能会从master那里
secondaries differ or the client may

1663
01:19:28,500 --> 01:19:31,200
得到过时的secondary
have a slightly stale indication from

1664
01:19:31,200 --> 01:19:33,000
得到过时的secondary
the master of which secondary to talk to

1665
01:19:33,000 --> 01:19:35,940
系统要么需要发送
the system either needs to send all

1666
01:19:35,940 --> 01:19:38,010
所有client端读取给primary服务器
client reads through the primary 

1667
01:19:38,010 --> 01:19:41,490
因为只有primary才可能知道实际发生了哪些操作
because only the primary is likely to know which

1668
01:19:41,490 --> 01:19:43,860
因为只有primary才可能知道实际发生了哪些操作
operations have really happened 

1669
01:19:43,860 --> 01:19:45,570
要么我们需要给secondary一个租约系统
or we need a lease system for the secondaries

1670
01:19:45,570 --> 01:19:47,400
就像primary那样
just like we have for the primary so

1671
01:19:47,400 --> 01:19:50,700
因此，众所周知
that it's well understood that when

1672
01:19:50,700 --> 01:19:55,030
当secondary可以或不能合法回应client时
secondary can or can't legally respond

1673
01:19:55,030 --> 01:19:56,650
我被告知，这些事情
a client and so these are the things I'm

1674
01:19:56,650 --> 01:19:58,570
必须在此系统中修复
aware of that would have to be fixed in this system 

1675
01:19:58,570 --> 01:20:00,550
增加了复杂性和技巧，使其具有很强的一致性
tor added complexity and

1676
01:20:00,550 --> 01:20:02,230
增加了复杂性和技巧，使其具有很强的一致性
chitchat to make it have strong consistency 

1677
01:20:02,230 --> 01:20:05,050
and you're actually the way

1678
01:20:05,050 --> 01:20:08,020
我通过思考lab内容得到这份列表
I got that list was by thinking about the labs 

1679
01:20:08,020 --> 01:20:09,940
你最终会做
you're gonna end up doing all

1680
01:20:09,940 --> 01:20:12,099
我刚才所说的所有事情
the things I just talked about as part

1681
01:20:12,099 --> 01:20:13,989
作为lab2，lab3的一部分
of labs two and three to build a

1682
01:20:13,989 --> 01:20:18,940
建立严格一致的系统
strictly consistent system okay so 

1683
01:20:18,940 --> 01:20:21,099
好吧，让我花一分钟的时间
let me spend one minute on there's actually

1684
01:20:21,099 --> 01:20:23,079
我在笔记中有一个链接
I have a link in the notes to a sort of

1685
01:20:23,079 --> 01:20:25,840
一个回顾性访谈，关于
retrospective interview about how well

1686
01:20:25,840 --> 01:20:28,389
GFS在Google生涯的前五年
GFS played out over the first five or

1687
01:20:28,389 --> 01:20:32,770
或十年中表现的如何出色
ten years of his life at Google 

1688
01:20:32,770 --> 01:20:36,219
高层次的总结是
so the high-level summary is that the most is

1689
01:20:36,219 --> 01:20:37,690
它取得了巨大的成功
that was tremendously successful and

1690
01:20:37,690 --> 01:20:40,570
许多Google的应用都使用了它
many many Google applications used it in

1691
01:20:40,570 --> 01:20:43,000
许多Google基础架构
a number of Google infrastructure was

1692
01:20:43,000 --> 01:20:45,250
被构建为一层
built as a layer like big file for

1693
01:20:45,250 --> 01:20:47,409
例如BigTable
example BigTable I mean was built as a

1694
01:20:47,409 --> 01:20:50,190
被构建为GFS之上的一层，MapReduce也是这样
layer on top of GFS and MapReduce also

1695
01:20:50,190 --> 01:20:54,550
在Google中被广泛使用
so widely used within Google 

1696
01:20:54,550 --> 01:20:57,460
可能最严重的局限是
may be the most serious limitation is that 

1697
01:20:57,460 --> 01:20:59,289
单一的master，而master必须为每个块，
there was a single master and the master had

1698
01:20:59,289 --> 01:21:01,510
每个文件都有一个表条目
to have a table entry for every file and

1699
01:21:01,510 --> 01:21:04,510
这意味着随着GFS的使用量增加，
every chunk and that mean does the GFS

1700
01:21:04,510 --> 01:21:06,820
它们涉及的文件越来越多
use grew and they're about more and more files 

1701
01:21:06,820 --> 01:21:08,650
主机就用完了内存
the master just ran out of memory

1702
01:21:08,650 --> 01:21:11,980
用完了RAM来存储文件
ran out of RAM to store the files and

1703
01:21:11,980 --> 01:21:13,690
你可以增加更多的RAM
you know you can put more RAM on but

1704
01:21:13,690 --> 01:21:15,010
但是一台计算机可以拥有多少RAM是有限度的
there's limits to how much RAM a single

1705
01:21:15,010 --> 01:21:18,309
但是一台计算机可以拥有多少RAM是有限度的
machine can have and so that was the

1706
01:21:18,309 --> 01:21:19,599
因此这是人们遇到的最直接的问题
most of the most immediate problem

1707
01:21:19,599 --> 01:21:24,159
此外，单个master服务器上的负载
people ran into in addition the load on

1708
01:21:24,159 --> 01:21:25,869
来自成千上万个client端的
a single master from thousands of clients 

1709
01:21:25,869 --> 01:21:28,030
这开始变得过大
started to be too much 

1710
01:21:28,030 --> 01:21:29,650
在master服务器中
in the master only, the cpu can only

1711
01:21:29,650 --> 01:21:30,940
cpu每秒只能处理数百个请求
process however many hundreds of

1712
01:21:30,940 --> 01:21:33,070
特别是硬盘的写操作
requests per second especially the write

1713
01:21:33,070 --> 01:21:35,739
很快就会有过多的client
things to disk and pretty soon there got

1714
01:21:35,739 --> 01:21:39,880
很快就会有过多的client
to be too many clients 

1715
01:21:39,880 --> 01:21:41,409
另一个问题是，一些应用程序发现
another problem with a some applications found it hard

1716
01:21:41,409 --> 01:21:44,260
很难处理这种有点奇怪的语义
to deal with this kind of sort of odd

1717
01:21:44,260 --> 01:21:47,500
最后一个问题是
semantics and a final problem is that

1718
01:21:47,500 --> 01:21:49,599
master服务器的故障切换并不是自动的
the master that was not an automatic

1719
01:21:49,599 --> 01:21:52,059
master服务器的故障切换并不是自动的
story for master failover

1720
01:21:52,059 --> 01:21:54,400
在我们阅读的GFS论文的原文里
in the original in the GFS paper as we

1721
01:21:54,400 --> 01:21:56,440
需要人工干预
read it like required human intervention

1722
01:21:56,440 --> 01:21:59,170
来处理已永久崩溃且需要更换的master主机
to deal with a master that had sort of

1723
01:21:59,170 --> 01:22:00,460
来处理已永久崩溃且需要更换的master主机
permanently crashed and needs to be replaced 

1724
01:22:00,460 --> 01:22:03,579
那可能要花几十分钟
and that could take tens of

1725
01:22:03,579 --> 01:22:05,980
甚至更长的时间
minutes or more I was just too long for

1726
01:22:05,980 --> 01:22:09,360
对于某些应用程序的故障恢复，时间太长了
failure recovery for some applications

1727
01:22:09,360 --> 01:22:13,630
好极了，我们星期四见
okay excellent I'll see you on Thursday

1728
01:22:13,630 --> 01:22:15,970
我们将听到这个更多课程主题的信息
and we'll hear more about all these

1729
01:22:15,970 --> 00:00:00,000
我们将听到这个更多课程主题的信息
themes over the semester

